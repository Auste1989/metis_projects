{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2. Part 1 (Scrape & Pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Selenium and BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get urls for all the pages and put them into a list\n",
    "def get_url_list():\n",
    "    \"\"\"Records the current url and goes through the website, clicking Next as many times as there are pages.\n",
    "    Returns a list of urls to be used in the get_htmls function.\"\"\"\n",
    "    #Creates a list of urls for all pages\n",
    "    url_list = []\n",
    "    \n",
    "    #Reads the url of the page the driver is currently in and adds it into the list\n",
    "    current_page_url = driver.current_url\n",
    "    url_list.append(current_page_url)\n",
    "    \n",
    "    count_except = 0\n",
    "    while True:\n",
    "        try:\n",
    "            #Find \"Next\" button and click it\n",
    "            search_button = driver.find_element_by_link_text('Next')\n",
    "            search_button.click()\n",
    "            current_page_url = driver.current_url\n",
    "            url_list.append(current_page_url)\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            #Count the exceptions\n",
    "            count_except += 1\n",
    "            #If the there haven't been 3 exceptions yet, sleep for a bit and then continue\n",
    "            if count_except < 3:\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                #If \"Next\" button isn't there anymore or an error occurs, return the list\n",
    "                #driver.close()\n",
    "                return url_list\n",
    "    #driver.close()\n",
    "    return url_list\n",
    "\n",
    "\n",
    "#Get all the html files for each car ad and put it into a list\n",
    "def get_htmls(url, html_list):\n",
    "    \"\"\"Takes an url (of one page) and an existing html list.\n",
    "    Returns updated html list with all the htmls from provided url.\"\"\"\n",
    "    #Convert the url into html and then soup it up\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html5lib')\n",
    "    \n",
    "    #Find all announcement items\n",
    "    all_ads = soup.find_all('a', class_ = 'announcement-item')\n",
    "    \n",
    "    #Copy the existing list of htmls\n",
    "    final_html_list = html_list\n",
    "    \n",
    "    for ad in all_ads:\n",
    "        html = ad.get('href')\n",
    "        final_html_list.append(html)\n",
    "    return final_html_list\n",
    "\n",
    "#Makes the soup from all of the html files\n",
    "def make_soup(html_list, soup_list):\n",
    "    \"\"\"Generates and return a soup object based on a given html list\"\"\"\n",
    "    final_soup_list = soup_list\n",
    "    for html in html_list:\n",
    "        source_code = requests.get(html)\n",
    "        soup = BeautifulSoup(source_code.text, 'html5lib')\n",
    "        final_soup_list.append(soup)\n",
    "    return final_soup_list\n",
    "\n",
    "#Get the make and engine size for each car\n",
    "def get_make_and_engine(soup):\n",
    "    \"\"\"Given an input soup object retrieves the advert title (containing Make_Model and Engine size)\n",
    "    Returns a dictionary with Make_Model and Engine keys\"\"\"\n",
    "    dividers = soup.find_all('div', {'class', 'col-5 classifieds-info'})\n",
    "    car = {}\n",
    "    for div in dividers:\n",
    "        for title in div('h1'):\n",
    "            title_list = title.text.split(',')\n",
    "            for i in range(3):\n",
    "                if i == 0:\n",
    "                    car['Make_Model'] = title_list[i]\n",
    "                elif i == 1:\n",
    "                    car['Engine_Size'] = title_list[i]\n",
    "    return car    \n",
    "\n",
    "\n",
    "#Get the rest of the features for each car\n",
    "def get_other_features(soup, car_dict):\n",
    "    \"\"\"Retrieves remaining features of a car using input soup object\n",
    "    Returns an updated cars dictionary\"\"\"\n",
    "    car = car_dict\n",
    "    params = soup.find_all('table', {'class', 'announcement-parameters'})\n",
    "    for param in params:\n",
    "        for tr in param('tr'):\n",
    "            for th in tr('th'):\n",
    "                col_title = th.text\n",
    "                for td in tr('td'):\n",
    "                    col_value = td.text\n",
    "                    car[col_title] = col_value\n",
    "    return car\n",
    "\n",
    "\n",
    "#Scrape all the features for all the cars\n",
    "def scrape(soup_list):\n",
    "    \"\"\"Given the soups file this function retrieves all the feature columns about each car\"\"\"\n",
    "    #For tracking\n",
    "    car_dict_list = []\n",
    "    for soup in soup_list:\n",
    "        #Retrieve the car make, model and engine size\n",
    "        car = get_make_and_engine(soup)\n",
    "        #Retrieve the rest of the features\n",
    "        car = get_other_features(soup, car)\n",
    "        car_dict_list.append(car)\n",
    "    return car_dict_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the website\n",
    "website = 'https://en.autoplius.lt/'\n",
    "\n",
    "##Could also have a website list\n",
    "#website_list = ['https://en.autoplius.lt/', 'https://en.autogidas.lt/']\n",
    "#for web in website_list:\n",
    "#    website = web\n",
    "#    driver.get(website)\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\"                     #  path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#Go to the website\n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now filter on cars (not minivans, or trucks)\n",
    "car_type_form = driver.find_element_by_id(\"cats_search_1\")\n",
    "car_type_form.send_keys(\"Cars\")\n",
    "car_type_form.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find \"Search\" button and click it\n",
    "search_button = driver.find_element_by_link_text('Search')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get all the urls\n",
    "url_list = get_url_list()\n",
    "print(len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_my_urls.pkl', 'wb') as picklefile2:\n",
    "    pickle.dump(url_list, picklefile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_my_urls.pkl', 'rb') as picklefile2:\n",
    "    url_list = pickle.load(picklefile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#It's to to get the htmls from each page (approx. 20 / page)\n",
    "html_list = []\n",
    "for url in url_list:\n",
    "    get_htmls(url, html_list)\n",
    "\n",
    "print(len(html_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at 600th html\n",
    "#html_list[600]\n",
    "#The html seems perfectly normal and works when I open it in the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make soup (yuck!)\n",
    "soup_list = []\n",
    "soup_list = make_soup(html_list, soup_list)\n",
    "print(len(soup_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the 600th soup\n",
    "soup_list[600]\n",
    "#AHA! Good job Watson :) The website adds something called <!-- DYNAMICTAGS --> \n",
    "#before the usual DOCTYPE after certain number of scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Scrape the hell out of this website!!!\n",
    "cars_dict = scrape(soup_list)\n",
    "len(cars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pickle this car list\n",
    "with open('cars_dict.pkl', 'wb') picklefile5:\n",
    "    pickle.dump(cars_dict, picklefile5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's unpickle this car list\n",
    "with open('cars_dict.pkl', 'rb') picklefile5:\n",
    "    my_cars_dict = pickle.load(picklefile5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the cars_dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many cars don't have Make_Model?\n",
    "# no_make = 0\n",
    "# no_make_indices = []\n",
    "# for i in range(len(cars_dict)):\n",
    "#     if 'Make_Model' not in list(cars_dict[i].keys()):\n",
    "#         no_make += 1\n",
    "#         no_make_indices.append(i)\n",
    "\n",
    "# print(str(no_make), \"cars don't have a Make.\")\n",
    "\n",
    "# #Which records are those?\n",
    "# no_make_indices\n",
    "# #Result - 600 onwards\n",
    "# #cars_dict[602]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create my dataset (pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now fill out the rows\n",
    "car_ad_dataset = pd.DataFrame(cars_dict)\n",
    "car_ad_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pickle the hell out of the result (don't want to have to run this again)\n",
    "with open('car_ad_dataset.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(car_ad_dataset, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
