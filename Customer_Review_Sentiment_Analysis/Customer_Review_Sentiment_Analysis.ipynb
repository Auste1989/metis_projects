{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration, Cleanup and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.050287Z",
     "start_time": "2018-09-02T18:26:38.045725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.061362Z",
     "start_time": "2018-09-02T18:26:38.051981Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#SQL related\n",
    "import sqlite3\n",
    "import pandas.io.sql as pd_sql\n",
    "\n",
    "#API related\n",
    "import requests\n",
    "\n",
    "#Preprocessing\n",
    "import re\n",
    "from geotext import GeoText\n",
    "from calendar import month_name\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Sentiment analysis\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Plotting fun\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#NLP fun\n",
    "import nltk\n",
    "from  collections  import namedtuple\n",
    "\n",
    "#Text cleaning (stemming, lemmatizing, etc.)\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import inflection\n",
    "from autocorrect import spell\n",
    "\n",
    "#Vectorizing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# Dimentionality reduction\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation, PCA\n",
    "\n",
    "#NLP clustering\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans, MeanShift, SpectralClustering, DBSCAN\n",
    "\n",
    "#Clustering help\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Clustering visualization\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Fun stuff\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.065083Z",
     "start_time": "2018-09-02T18:26:38.062997Z"
    }
   },
   "outputs": [],
   "source": [
    "#'/Users/auste_m/ds/metis/metisgh/github/metis_projects/Customer_Review_Sentiment_Analysis/Datasets/twitter-airline-sentiment/database.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.070170Z",
     "start_time": "2018-09-02T18:26:38.067069Z"
    }
   },
   "outputs": [],
   "source": [
    "#Setting up for working with SQLite database\n",
    "sqlite_file = './Datasets/twitter-airline-sentiment/database.sqlite'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.077330Z",
     "start_time": "2018-09-02T18:26:38.071989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of the table are: \n",
      "['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone']\n",
      "\n",
      "Preview of one of the rows in the table:\n",
      "(567588278875213824, 'neutral', 1, '', '', 'Delta', '', 'JetBlueNews', '', 0, \"@JetBlue's new CEO seeks the right balance to please passengers and Wall ... - Greenfield Daily Reporter http://t.co/LM3opxkxch\", '', '2015-02-16 23:36:05 -0800', 'USA', 'Sydney')\n"
     ]
    }
   ],
   "source": [
    "#Check one of the rows in the table\n",
    "preview = cursor.execute(\"SELECT * FROM Tweets LIMIT 20\")\n",
    "columns = [column[0] for column in preview.description]\n",
    "print('The columns of the table are:' + ' \\n' + str(columns) + '\\n')\n",
    "print('Preview of one of the rows in the table:' + '\\n' + str(preview.fetchone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.123433Z",
     "start_time": "2018-09-02T18:26:38.079515Z"
    }
   },
   "outputs": [],
   "source": [
    "#Retrieve relevant information from Tweets table in SQLite database and store them in a pandas dataframe\n",
    "query = \"\"\"SELECT airline, retweet_count, text as 'tweet' \n",
    "            FROM Tweets\"\"\"\n",
    "\n",
    "tweets_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.139593Z",
     "start_time": "2018-09-02T18:26:38.125289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14485 entries, 0 to 14484\n",
      "Data columns (total 3 columns):\n",
      "airline          14485 non-null object\n",
      "retweet_count    14485 non-null int64\n",
      "tweet            14485 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 339.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delta</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue's new CEO seeks the right balance to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united yes. We waited in line for almost an h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united the we got into the gate at IAH on tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir its cool that my bags take a bit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline  retweet_count                                              tweet\n",
       "0      Delta              0  @JetBlue's new CEO seeks the right balance to ...\n",
       "1      Delta              0  @JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ ...\n",
       "2     United              0  @united yes. We waited in line for almost an h...\n",
       "3     United              0  @united the we got into the gate at IAH on tim...\n",
       "4  Southwest              0  @SouthwestAir its cool that my bags take a bit..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "print(tweets_df.info())\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put aside a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.149405Z",
     "start_time": "2018-09-02T18:26:38.142215Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_test = tweets_df[10000:12000]\n",
    "tweets_train = tweets_df[:10000]\n",
    "tweets_train = tweets_train.append(tweets_df[12000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.153184Z",
     "start_time": "2018-09-02T18:26:38.150903Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./pickles/tweets_test1.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(tweets_test, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:38.157410Z",
     "start_time": "2018-09-02T18:26:38.155103Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tweets_train.info()\n",
    "# tweets_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's gather airport information from an external API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:40.229391Z",
     "start_time": "2018-09-02T18:26:38.159253Z"
    }
   },
   "outputs": [],
   "source": [
    "#First need to get global airport database through an API request \n",
    "airport_db_url = 'https://aviation-edge.com/api/public/airportDatabase?key=42e87b-a2f1be-c446fa-06d7a2-012f14'\n",
    "get_response = requests.get(airport_db_url)\n",
    "airport_db = get_response.json()\n",
    "\n",
    "#Then I need to extract the information that is relevant to me (airport names and codes)\n",
    "airport_info = []\n",
    "\n",
    "for airport in airport_db:\n",
    "    airport_info.append(airport['codeIataAirport'])\n",
    "    airport_info.append(airport['nameAirport'])\n",
    "\n",
    "#Test that results make sense\n",
    "# if 'IAH' in airport_codes:\n",
    "#     print(airport_db[airport_codes.index('IAH')])\n",
    "# else:\n",
    "#     print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:40.235227Z",
     "start_time": "2018-09-02T18:26:40.231375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20102"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airport_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's set up some helped functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:40.259676Z",
     "start_time": "2018-09-02T18:26:40.237578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:6: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:33: DeprecationWarning: invalid escape sequence \\d\n",
      "<input>:34: DeprecationWarning: invalid escape sequence \\d\n",
      "<input>:49: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:6: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:33: DeprecationWarning: invalid escape sequence \\d\n",
      "<input>:34: DeprecationWarning: invalid escape sequence \\d\n",
      "<input>:49: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:6: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "<input>:33: DeprecationWarning: invalid escape sequence \\d\n",
      "<input>:34: DeprecationWarning: invalid escape sequence \\d\n",
      "<input>:49: DeprecationWarning: invalid escape sequence \\w\n",
      "<ipython-input-80-424d8eab59cc>:6: DeprecationWarning: invalid escape sequence \\w\n",
      "  pattern1 = re.compile('@[A-Za-z]+\\w')\n",
      "<ipython-input-80-424d8eab59cc>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "  pattern2 = re.compile('#\\w+')\n",
      "<ipython-input-80-424d8eab59cc>:33: DeprecationWarning: invalid escape sequence \\d\n",
      "  pattern3 = re.compile('[A-Z]?\\d+[A-Z]+')\n",
      "<ipython-input-80-424d8eab59cc>:34: DeprecationWarning: invalid escape sequence \\d\n",
      "  pattern4 = re.compile('\\d+')\n",
      "<ipython-input-80-424d8eab59cc>:49: DeprecationWarning: invalid escape sequence \\w\n",
      "  pattern5 = re.compile('http://t.co/\\w+')\n"
     ]
    }
   ],
   "source": [
    "#helper function to remove stuff from tweets\n",
    "\n",
    "def remove_airline(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with hashtag removed.\"\"\"\n",
    "    pattern1 = re.compile('@[A-Za-z]+\\w')\n",
    "    new_string = string\n",
    "    try:\n",
    "        all_airlines = pattern1.findall(new_string)\n",
    "        for airline in all_airlines:\n",
    "            new_string = re.sub(airline, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "def remove_hashtag(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with hashtag removed.\"\"\"\n",
    "    pattern2 = re.compile('#\\w+')\n",
    "    new_string = string\n",
    "    try:\n",
    "        all_hashtags = pattern2.findall(new_string)\n",
    "        for hashtag in all_hashtags:\n",
    "            new_string = re.sub(hashtag, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def remove_code(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with any capital letter & digit combination text removed.\"\"\"\n",
    "    pattern3 = re.compile('[A-Z]?\\d+[A-Z]+')\n",
    "    pattern4 = re.compile('\\d+')\n",
    "    new_string = string\n",
    "    try:\n",
    "        codes = pattern3.findall(new_string)\n",
    "        codes.extend(pattern4.findall(new_string))\n",
    "        for elem in codes:\n",
    "            new_string = re.sub(elem, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "    \n",
    "    \n",
    "def remove_url(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with any urls removed removed.\"\"\"\n",
    "    pattern5 = re.compile('http://t.co/\\w+')\n",
    "    new_string = string\n",
    "    try:\n",
    "        urls = pattern5.findall(new_string)\n",
    "        for url in urls:\n",
    "            new_string = re.sub(url, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def remove_location(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with location information removed.\"\"\"\n",
    "    new_string = string\n",
    "    geo_loc = GeoText(string)\n",
    "    locations = []\n",
    "    if geo_loc.cities != []:\n",
    "        locations.extend(geo_loc.cities)\n",
    "    if geo_loc.countries != []:\n",
    "        locations.extend(geo_loc.countries)\n",
    "    try:\n",
    "        for loc in locations:\n",
    "            new_string = re.sub(loc, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "    \n",
    "    \n",
    "def remove_month(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with month information removed.\"\"\"\n",
    "    new_string = string\n",
    "    all_months = month_name[1:]\n",
    "    try:\n",
    "        for word in string.split():\n",
    "            if word in all_months:\n",
    "                new_string = re.sub(word, '', new_string)\n",
    "            else:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with emojis removed.\"\"\"    \n",
    "    pattern6 = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    new_string = string\n",
    "    try:\n",
    "        emojis = pattern6.findall(new_string)\n",
    "        for emoji in emojis:\n",
    "            new_string = re.sub(emoji, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "def remove_airport(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with airport codes removed.\"\"\" \n",
    "    new_string = string\n",
    "    try:\n",
    "        for word in string.split():\n",
    "            if word in airport_info:\n",
    "                new_string = re.sub(word, '', new_string)\n",
    "            else:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "\n",
    "def get_clean_tweet(string):\n",
    "    \"\"\"Takes a string and uses all the cleaning related functions above to remove unnecessary elements.\n",
    "    Returns a 'clean' string.\"\"\"\n",
    "    clean_string = remove_airline(remove_hashtag(remove_code(remove_url(remove_location(remove_month(remove_emoji(remove_airport(string))))))))\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "def get_Vader_sentiment(string):\n",
    "    \"\"\"Takes a string as input. Uses text processing mashape API to retrieve the sentiment.\n",
    "    Returns a sentiment label and score (from -1 to 1, negatives signalling negative sentiment).\"\"\" \n",
    "    SIA = SentimentIntensityAnalyzer()\n",
    "    total_score = SIA.polarity_scores(string)\n",
    "    if total_score['compound'] < -0.05:\n",
    "        sentiment = 'negative'\n",
    "    elif total_score['compound'] >= 0.2:\n",
    "        sentiment = 'positive'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    polarity = total_score['compound']\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def get_TextBlob_sentiment(string):\n",
    "    \"\"\"Takes a string as input. Uses text processing mashape API to retrieve the sentiment.\n",
    "    Returns a sentiment label and score (from -1 to 1, negatives signalling negative sentiment).\"\"\" \n",
    "    sentiment_all = TextBlob(string).sentiment\n",
    "    if sentiment_all.polarity < -0.05:\n",
    "        sentiment = 'negative'\n",
    "    elif sentiment_all.polarity >= 0.2:\n",
    "        sentiment = 'positive'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    polarity = sentiment_all.polarity\n",
    "    return sentiment\n",
    "\n",
    "def get_sentiment(string):\n",
    "    \"\"\"Takes a string, generates sentiment using TextBlob and Vader.\n",
    "    Returns the final sentiment label based on the two techniques (negative in either => negative)\"\"\"\n",
    "    sentiment_blob = get_TextBlob_sentiment(string)\n",
    "    sentiment_vader = get_Vader_sentiment(string)\n",
    "    sentiment = ''\n",
    "    if sentiment_blob == 'negative' or sentiment_vader == 'negative':\n",
    "        sentiment = 'negative'\n",
    "    elif sentiment_blob == 'neutral' or sentiment_vader == 'neutral':\n",
    "        sentiment = 'neutral'\n",
    "    else:\n",
    "        sentiment = 'positive'\n",
    "    return sentiment\n",
    "\n",
    "# def get_sentiment_API(string):\n",
    "#     \"\"\"Takes a string as input. Uses text processing mashape API to retrieve the sentiment.\n",
    "#     Returns a sentiment label and score (from -1 to 1, negatives signalling negative sentiment).\"\"\" \n",
    "#     #API stuff\n",
    "#     sentiment_API_url = 'https://japerk-text-processing.p.mashape.com/sentiment/'\n",
    "#     sentiment_API_response = requests.post(sentiment_API_url,\n",
    "#                                           data={\n",
    "#                                             \"language\": \"english\",\n",
    "#                                             \"text\": test_tweet}\n",
    "#                                               ,\n",
    "#                                           headers={\n",
    "#                                             \"X-Mashape-Key\": \"3hN4k8H8Brmsh0Hp4sTefboY6vHpp1qZZ3jjsnvlGiMsNSK59o\",\n",
    "#                                             \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "#                                             \"Accept\": \"application/json\"}\n",
    "#                                           )\n",
    "\n",
    "#     sentiment_all = sentiment_API_response.json()\n",
    "#     sentiment = sentiment_all['label']\n",
    "#     neg_prob = sentiment_all['probability']['neg']\n",
    "#     return (sentiment, neg_prob)\n",
    "\n",
    "def simple_pos(pos):\n",
    "    if pos == 'NOUN':\n",
    "        return 'n'\n",
    "    elif pos == 'VERB':\n",
    "        return 'v'\n",
    "    elif pos == 'ADJ':\n",
    "        return 'a'\n",
    "    elif pos == 'ADV':\n",
    "        return 'r'\n",
    "    elif pos == 'ADJ_SAT':\n",
    "        return 's'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def process_words(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the string after lemmatizing and singularizing the words.\"\"\"\n",
    "    processed_string = string\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    word_pos_pair = pos_tag(word_tokenize(processed_string), tagset='universal')\n",
    "    for word, pos in word_pos_pair:\n",
    "        word_pos = simple_pos(pos)\n",
    "        #spell_correct = ' '+spell(word) if \"'\" in word else spell(word)\n",
    "        lame_word = lmtzr.lemmatize(word, word_pos)\n",
    "        singular_word = inflection.singularize(lame_word)\n",
    "        processed_string = processed_string.replace(word, singular_word)\n",
    "    return processed_string \n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "def get_issue_name(issue_index):\n",
    "    \"\"\"Takes the cluster index.\n",
    "    Returns the issue name.\"\"\"\n",
    "    cluster_name_list = ['Issue with Contacting Customer Service', 'Poor Customer Service', 'Missed Connection', 'Cancellation',\n",
    "                    'Boarding / Deplaning Issue', 'Rebooking Issues', 'Poor Customer Service / Rebooking Issues', 'Lost Baggage', 'Bad Weather Related',\n",
    "                    'Poor Customer / Airport Agent Service']\n",
    "    cluster_index = int(issue_index)\n",
    "    issue = cluster_name_list[cluster_index]\n",
    "    return issue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:44.497083Z",
     "start_time": "2018-09-02T18:26:40.262701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tweet: '@SouthwestAir need to learn how to treat people with respect and just a little dignity. #FAIL '\n",
      "Scores using TextBlob default Sentiment(polarity=-0.34375, subjectivity=0.39999999999999997)\n",
      "Scores using TextBlob Naive Bayes classifier Sentiment(classification='pos', p_pos=0.7744402760303103, p_neg=0.22555972396969037)\n",
      "Scores using Vader {'neg': 0.0, 'neu': 0.594, 'pos': 0.406, 'compound': 0.8024}\n"
     ]
    }
   ],
   "source": [
    "#Are the tweets positive or negative (tried TextBlob.sentiment, TextBlob.NaiveBayesAnalyzer, nltk.Vader, API sentiment analyzer)\n",
    "\n",
    "test_tweet = tweets_train['tweet'][14063]\n",
    "label_TB = TextBlob(test_tweet).sentiment\n",
    "label_NB = TextBlob(test_tweet, analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "total_score = SIA.polarity_scores(test_tweet)\n",
    "print(\"Example tweet: '\"+ str(test_tweet), \"'\")\n",
    "print('Scores using TextBlob default', str(label_TB))\n",
    "print('Scores using TextBlob Naive Bayes classifier', str(label_NB))\n",
    "print('Scores using Vader', str(total_score))\n",
    "# print('Label and negative probability using sentiment API', str(get_sentiment_API(test_tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the discrepancies between TextBlob and Vader approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:44.502151Z",
     "start_time": "2018-09-02T18:26:44.499297Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing different approaches\n",
    "# tweets_train['sentiment_TextBlob'] = [get_TextBlob_sentiment(tweet)[0] for tweet in tweets_train['tweet']]\n",
    "# tweets_train['polarity_TextBlob'] = [get_TextBlob_sentiment(tweet)[1] for tweet in tweets_train['tweet']]\n",
    "# tweets_train['sentiment_Vader'] = [get_Vader_sentiment(tweet)[0] for tweet in tweets_train['tweet']]\n",
    "# tweets_train['polarity_Vader'] = [get_Vader_sentiment(tweet)[1] for tweet in tweets_train['tweet']]\n",
    "# tweets_train['sentiment'] = [get_sentiment_API(tweet)[0] for tweet in tweets_train['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:44.506338Z",
     "start_time": "2018-09-02T18:26:44.504175Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweets_train[(tweets_train['sentiment_TextBlob'] == 'positive') & (tweets_train['sentiment_Vader'] == 'negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:44.510299Z",
     "start_time": "2018-09-02T18:26:44.508218Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweets_train[(tweets_train['sentiment_Vader'] == 'positive') & (tweets_train['sentiment_TextBlob'] == 'negative')].sort_values(by='polarity_Vader', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:26:44.514630Z",
     "start_time": "2018-09-02T18:26:44.512321Z"
    }
   },
   "outputs": [],
   "source": [
    "#tweets_train['tweet'][14063]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.216670Z",
     "start_time": "2018-09-02T18:26:44.516206Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adding a column for sentiment label and negative score each\n",
    "tweets_train['sentiment'] = [get_sentiment(tweet) for tweet in tweets_train['tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm only really interested in negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.230839Z",
     "start_time": "2018-09-02T18:28:30.219085Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5192 entries, 1 to 14480\n",
      "Data columns (total 4 columns):\n",
      "airline          5192 non-null object\n",
      "retweet_count    5192 non-null int64\n",
      "tweet            5192 non-null object\n",
      "sentiment        5192 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 202.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Let's filter only on negative tweets (since that is our constructive criticism (or hopefully so))\n",
    "neg_tweets = tweets_train[tweets_train['sentiment'] == 'negative']\n",
    "neg_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get sweeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T21:01:51.536234Z",
     "start_time": "2018-08-17T21:01:51.533982Z"
    }
   },
   "source": [
    "#### Examining hashtag containing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.235918Z",
     "start_time": "2018-09-02T18:28:30.233110Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Identify all the tweets containing hashtags\n",
    "# pattern2 = re.compile('#[A-Za-z]+\\w')\n",
    "# count_hash_tweets = 0\n",
    "\n",
    "# for index, tweet in enumerate(tweets_train['tweet']):\n",
    "#     try:\n",
    "#         h_tweet = pattern2.search(tweet).group()\n",
    "#         print(index, h_tweet)\n",
    "#         count_hash_tweets += 1\n",
    "#     except:\n",
    "#         continue\n",
    "        \n",
    "# print('\\nTotal number of tweets containing hashtags =', str(count_hash_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same for urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.239812Z",
     "start_time": "2018-09-02T18:28:30.237446Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Find url pattern\n",
    "# https_list = []\n",
    "\n",
    "# for tweet in tweets_train['tweet']:\n",
    "#     if url_remove(tweet) == []:\n",
    "#         pass\n",
    "#     else:\n",
    "#         https_list.append(url_remove(tweet))\n",
    "        \n",
    "# print(https_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.244053Z",
     "start_time": "2018-09-02T18:28:30.241400Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Testing code_remove function\n",
    "# test_string = tweets_train['tweet'][1582]\n",
    "# print(test_string)\n",
    "# print(url_remove(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wooohooo!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's location time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.249511Z",
     "start_time": "2018-09-02T18:28:30.246517Z"
    }
   },
   "outputs": [],
   "source": [
    "# madrid_tweet = tweets_train['tweet'][14451]\n",
    "# geo = GeoText(madrid_tweet)\n",
    "# geo.cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:30.254645Z",
     "start_time": "2018-09-02T18:28:30.251796Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clean up tweet column, remove the \"@word\" from the rest of the tweet\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet'].apply(remove_airline)\n",
    "\n",
    "# #Clean up tweet column, remove the hashtags from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_hashtag)\n",
    "\n",
    "# #Clean up tweet column, remove code-like elements from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_code)\n",
    "\n",
    "# #Clean up tweet column, remove urls from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_url)\n",
    "\n",
    "# #Clean up tweet column, remove locations from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_location)\n",
    "\n",
    "# #Clean up tweet column, remove month names from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_month)\n",
    "\n",
    "# #Clean up tweet column, remove emojis from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_emoji)\n",
    "\n",
    "# #Clean up tweet column, remove airport codes and names from all tweets\n",
    "# tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:56.887623Z",
     "start_time": "2018-09-02T18:28:30.256721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Clean up tweet column, remove codes, hashtags, airline names, urls, locations, etc.\n",
    "neg_tweets['tweet_clean'] = neg_tweets['tweet'].apply(get_clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini moment of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:56.895309Z",
     "start_time": "2018-09-02T18:28:56.889910Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ #nothappy \n",
      "\n",
      " is REALLY getting on my nerves !!   \n",
      "\n",
      "@united the we got into the gate at IAH on time and have given our seats and closed the flight. If you know people is arriving, have to wait \n",
      "\n",
      " the we got into the gate at  on time and have given our seats and closed the flight. If you know people is arriving, have to wait \n",
      "\n",
      "@united lots of reports of system failures delaying flights over the last week. Currently sitting on the tarmac at OGG for over an hour. \n",
      "\n",
      " lots of reports of system failures delaying flights over the last week. Currently sitting on the tarmac at  for over an hour. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's make sure it works (indexes to test = 1, 3, 12805)\n",
    "print(neg_tweets['tweet'][1], '\\n')\n",
    "print(neg_tweets['tweet_clean'][1], '\\n')\n",
    "\n",
    "print(neg_tweets['tweet'][3], '\\n')\n",
    "print(neg_tweets['tweet_clean'][3], '\\n')\n",
    "\n",
    "print(neg_tweets['tweet'][12805], '\\n')\n",
    "print(neg_tweets['tweet_clean'][12805], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:56.916269Z",
     "start_time": "2018-09-02T18:28:56.898768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>is REALLY getting on my nerves !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united yes. We waited in line for almost an h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>yes. We waited in line for almost an hour to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united the we got into the gate at IAH on tim...</td>\n",
       "      <td>negative</td>\n",
       "      <td>the we got into the gate at  on time and have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united I like delays less than you because I'...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I like delays less than you because I'm the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united, link to current status of flights/air...</td>\n",
       "      <td>negative</td>\n",
       "      <td>, link to current status of flights/airports? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united I tried 2 DM it would not go thru... n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I tried  DM it would not go thru... not sure why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united i have items of sentimental value that...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i have items of sentimental value that I'm he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir We have been stuck in SJU for se...</td>\n",
       "      <td>negative</td>\n",
       "      <td>We have been stuck in  for several hours and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@United is officially the worst, most delayed,...</td>\n",
       "      <td>negative</td>\n",
       "      <td>is officially the worst, most delayed, and le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir won't answer their phones #Horri...</td>\n",
       "      <td>negative</td>\n",
       "      <td>won't answer their phones</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline  retweet_count  \\\n",
       "1       Delta              0   \n",
       "2      United              0   \n",
       "3      United              0   \n",
       "6      United              0   \n",
       "7      United              0   \n",
       "9      United              0   \n",
       "16     United              0   \n",
       "17  Southwest              0   \n",
       "21     United              0   \n",
       "22  Southwest              0   \n",
       "\n",
       "                                                tweet sentiment  \\\n",
       "1   @JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ ...  negative   \n",
       "2   @united yes. We waited in line for almost an h...  negative   \n",
       "3   @united the we got into the gate at IAH on tim...  negative   \n",
       "6   @united I like delays less than you because I'...  negative   \n",
       "7   @united, link to current status of flights/air...  negative   \n",
       "9   @united I tried 2 DM it would not go thru... n...  negative   \n",
       "16  @united i have items of sentimental value that...  negative   \n",
       "17  @SouthwestAir We have been stuck in SJU for se...  negative   \n",
       "21  @United is officially the worst, most delayed,...  negative   \n",
       "22  @SouthwestAir won't answer their phones #Horri...  negative   \n",
       "\n",
       "                                          tweet_clean  \n",
       "1                 is REALLY getting on my nerves !!    \n",
       "2    yes. We waited in line for almost an hour to ...  \n",
       "3    the we got into the gate at  on time and have...  \n",
       "6    I like delays less than you because I'm the o...  \n",
       "7   , link to current status of flights/airports? ...  \n",
       "9    I tried  DM it would not go thru... not sure why  \n",
       "16   i have items of sentimental value that I'm he...  \n",
       "17   We have been stuck in  for several hours and ...  \n",
       "21   is officially the worst, most delayed, and le...  \n",
       "22                      won't answer their phones      "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving (just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:56.920127Z",
     "start_time": "2018-09-02T18:28:56.918112Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Pickle the dataset, just in case \n",
    "# with open('./pickles/negative_tweets_new.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(neg_tweets, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to the modelling part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:56.930556Z",
     "start_time": "2018-09-02T18:28:56.922085Z"
    }
   },
   "outputs": [],
   "source": [
    "#Moving forward read from pickle rather than repeat above steps\n",
    "with open('./pickles/negative_tweets.pkl', 'rb') as picklefile:\n",
    "    negative_tweets = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:28:56.942317Z",
     "start_time": "2018-09-02T18:28:56.932463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>is REALLY getting on my nerves !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "      <td>yes. We waited in line for almost an hour to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United</td>\n",
       "      <td>the we got into the gate at  on time and have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>United</td>\n",
       "      <td>I like delays less than you because I'm the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United</td>\n",
       "      <td>, link to current status of flights/airports? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline                                        tweet_clean\n",
       "1   Delta                is REALLY getting on my nerves !!  \n",
       "2  United   yes. We waited in line for almost an hour to ...\n",
       "3  United   the we got into the gate at  on time and have...\n",
       "6  United   I like delays less than you because I'm the o...\n",
       "7  United  , link to current status of flights/airports? ..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets = negative_tweets.drop(columns=['tweet', 'retweet_count', 'sentiment'])\n",
    "negative_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:07.529786Z",
     "start_time": "2018-09-02T18:28:56.944079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' be REALLY get on my nerve !!  ',\n",
       " ' ye. We wait in line for almost an hour to do so. Some passenger just leave not want to wait past be.',\n",
       " ' the we get into the gate at  on time and have give our seat and close the flight. If you know person be arrive, have to wait',\n",
       " \" I like delay less than you because I'm the one on the plane. Connect me with a voucher\",\n",
       " \", link to current status of flights/airport? Fly BWI-EWR-MCO thi morning yet can't yet tell what any problem be except see snow.\",\n",
       " ' I try  DM it would not go thru... not sure why',\n",
       " \" i have item of sentimental value that I'm heartbroken be miss\",\n",
       " ' We have be stick in  for several hour and no one be answer here. Really tough to  SW. No response be bad.',\n",
       " ' be officially the bad, most delay, and least helpful airline I have ever have the mbefortune of fly on',\n",
       " \" won't answer their phone    \"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some prep work (lemmatize)\n",
    "tweet_col = negative_tweets['tweet_clean']\n",
    "tweet_col = [process_words(tweet) for tweet in tweet_col]\n",
    "tweet_col[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:07.536956Z",
     "start_time": "2018-09-02T18:29:07.531978Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some prep work (removing some of the most popular, but not indicative words)\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"', '!', ':', 'DM', 'dm', 'Dm', 'get', 'amp', 'yes', 'no', 'thi', 'ua', 'go', 'airline', 'every',\n",
    "         'flight', 'flighting', 'fly', 'flightled', 'flighted', 'flightr', 'flightlation', 'ever', 'never',\n",
    "         'minute', 'min', 'hour', 'hr', 'day', 'today', 'tonight', 'tomorrow', 'time', 'first', 'last',]\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:07.542317Z",
     "start_time": "2018-09-02T18:29:07.539335Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "            'strip_accents': 'ascii',\n",
    "            'lowercase': True,\n",
    "            'stop_words': stop, \n",
    "            'min_df': 5,\n",
    "            'ngram_range': (1, 2)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:07.794780Z",
     "start_time": "2018-09-02T18:29:07.544220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>aa</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>accept</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>access</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>...</th>\n",
       "      <th>year old</th>\n",
       "      <th>yell</th>\n",
       "      <th>yep</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>youre</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  aa  able  absolute  absolutely  absurd  accept  acceptable  access  \\\n",
       "0   0   0     0         0           0       0       0           0       0   \n",
       "1   0   0     0         0           0       0       0           0       0   \n",
       "2   0   0     0         0           0       0       0           0       0   \n",
       "3   0   0     0         0           0       0       0           0       0   \n",
       "4   0   0     0         0           0       0       0           0       0   \n",
       "\n",
       "   accommodate  ...   year old  yell  yep  yesterday  yet  yo  youre  yr  \\\n",
       "0            0  ...          0     0    0          0    0   0      0   0   \n",
       "1            0  ...          0     0    0          0    0   0      0   0   \n",
       "2            0  ...          0     0    0          0    0   0      0   0   \n",
       "3            0  ...          0     0    0          0    0   0      0   0   \n",
       "4            0  ...          0     0    0          0    2   0      0   0   \n",
       "\n",
       "   yr old  zero  \n",
       "0       0     0  \n",
       "1       0     0  \n",
       "2       0     0  \n",
       "3       0     0  \n",
       "4       0     0  \n",
       "\n",
       "[5 rows x 1608 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to convert words to vectors, cause the computer doesn't understand words like we, humans, do\n",
    "count_vec = CountVectorizer(**params)\n",
    "negative_tweets_cv_viz = count_vec.fit_transform(tweet_col)\n",
    "negative_tweets_cv = negative_tweets_cv_viz.toarray()\n",
    "negative_tweets_cv = pd.DataFrame(negative_tweets_cv, columns=count_vec.get_feature_names())\n",
    "negative_tweets_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:07.799647Z",
     "start_time": "2018-09-02T18:29:07.796865Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of topics\n",
    "n_topics = 9\n",
    "n_words = 7       #originally 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:08.972934Z",
     "start_time": "2018-09-02T18:29:07.801787Z"
    }
   },
   "outputs": [],
   "source": [
    "#Best results were generated by CountVectorizer + NMF\n",
    "nmf_cv_model = NMF(n_components=n_topics)\n",
    "nmf_cv_data = nmf_cv_model.fit_transform(negative_tweets_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T18:29:08.981832Z",
     "start_time": "2018-09-02T18:29:08.974749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer and NMF:\n",
      "\n",
      "Topic  0\n",
      "service, customer, customer service, terrible, worst, poor, worst customer\n",
      "\n",
      "Topic  1\n",
      "cancelled, hold, one, rebook, weather, due, refund\n",
      "\n",
      "Topic  2\n",
      "delay, due, miss, connection, delay due, crew, weather\n",
      "\n",
      "Topic  3\n",
      "bag, lose, check, still, lose bag, baggage, late\n",
      "\n",
      "Topic  4\n",
      "plane, sit, late, leave, one, sit plane, gate\n",
      "\n",
      "Topic  5\n",
      "call, back, late, call back, tell, hold, try\n",
      "\n",
      "Topic  6\n",
      "bad, weather, experience, make, bad weather, bad experience, thank\n",
      "\n",
      "Topic  7\n",
      "problem, booking problem, booking, help, seat, need, reflight\n",
      "\n",
      "Topic  8\n",
      "wait, gate, agent, still, gate agent, phone, hold\n"
     ]
    }
   ],
   "source": [
    "print('Count vectorizer and NMF:')\n",
    "display_topics(nmf_cv_model, count_vec.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.118Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_cv_tsne = TSNE(n_components = 2, perplexity = 50.0)\n",
    "nmf_cv_sne = nmf_cv_tsne.fit_transform(nmf_cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.119Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)  \n",
    " \n",
    "plt.scatter(nmf_cv_sne[:, 0], nmf_cv_sne[:, 1], alpha=0.5)\n",
    "plt.title('tSNE on NMF CV model (topics = 9, words = 7)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Not too bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's save the model and the dimensionally reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.121Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./pickles/count_vec_new.pkl', 'wb') as picklefile_cv:\n",
    "#     pickle.dump(count_vec, picklefile_cv)\n",
    "\n",
    "# with open('./pickles/nmf_dim_reduction_new.pkl', 'wb') as picklefile_nmf:\n",
    "#     pickle.dump(nmf_cv_model, picklefile_nmf)\n",
    "\n",
    "# with open('./pickles/vectorized_dataset_new.pkl', 'wb') as picklefile_d:\n",
    "#     pickle.dump(nmf_cv_data, picklefile_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now continue working from the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.123Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./pickles/count_vec.pkl', 'rb') as picklefile_cv:\n",
    "    count_vec = pickle.load(picklefile_cv)\n",
    "\n",
    "with open('./pickles/nmf_dim_reduction.pkl', 'rb') as picklefile_nmf:\n",
    "    nmf_dim_reduction = pickle.load(picklefile_nmf)\n",
    "\n",
    "with open('./pickles/vectorized_dataset.pkl', 'rb') as picklefile_d:\n",
    "    vectorized_dataset = pickle.load(picklefile_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.125Z"
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.126Z"
    }
   },
   "outputs": [],
   "source": [
    "km_classifier = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "km_classifier.fit(vectorized_dataset)\n",
    "\n",
    "final_tsne = TSNE(n_components = 2, perplexity = 50.0)\n",
    "final_sne = final_tsne.fit_transform(vectorized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.130Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in list(km_classifier.labels_):\n",
    "    cluster_names = get_issue_name(i)\n",
    "    \n",
    "cluster_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.131Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.title('Clustering with KMeans')\n",
    "sns.scatterplot(final_sne[:, 0], final_sne[:, 1], hue = km_classifier.labels_, legend ='full', palette='rainbow');\n",
    "# plt.savefig('/Users/auste_m/ds/metis/metisgh/github/metis_projects/Customer_Review_Sentiment_Analysis/tSNE.png', format='png', dpi=120, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.133Z"
    }
   },
   "outputs": [],
   "source": [
    "#Topic list in the order displayed after Count Vectorizer and NMF (not corresponding to clusters yet)\n",
    "topic_list = ['Poor Customer Service', 'Cancellation due to weather', 'Missed Connection', 'Lost Baggage', 'Boarding / Deplaning Issue',\n",
    "                   'Issue with Contacting Customer Service', 'Bad Weather Related', 'Rebooking Issues', 'Poor Service from Airport Agent(s)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.134Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.figure(dpi=150)\n",
    "    sns.scatterplot(final_sne[:, 0], final_sne[:, 1], hue=vectorized_dataset[:,i], legend=None, palette='rainbow')\n",
    "    plt.title('Topic ' + str(topic_list[i]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.137Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_list_in_order = ['Issue with Contacting Customer Service', 'Poor Customer Service', 'Missed Connection', 'Cancellation due to weather',\n",
    "                       'Poor Customer Service', 'Boarding / Deplaning Issue', 'Rebooking Issues', 'Lost Baggage', 'Bad Weather Related', \n",
    "                       'Poor Service from Airport Agent(s)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.138Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./pickles/km_classifier_new.pkl', 'wb') as picklefile_c:\n",
    "#     pickle.dump(km_classifier, picklefile_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the best model in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.142Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./pickles/tweets_test.pkl', 'rb') as test_set:\n",
    "    tweets_test = pickle.load(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.143Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_issue(string):\n",
    "    \"\"\"Takes a string (tweet).\n",
    "    Returns an issue class it belongs to (based on the pretrained model).\"\"\"\n",
    "    with open('km_classifier.pkl', 'rb') as model:\n",
    "        km_classifier = pickle.load(model)\n",
    "    processed_tweet = process_words(get_clean_tweet(string))\n",
    "    vectorized_tweet = count_vec.transform([processed_tweet])\n",
    "    ready_tweet = nmf_dim_reduction.transform(vectorized_tweet)\n",
    "    issue = km_classifier.predict(ready_tweet)\n",
    "    return issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Et Voila! Let's recommend some actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.145Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_suggestion(string):\n",
    "    \"\"\"Takes a string, puts it through a pipeline of actions:\n",
    "            1. Defines the sentiment (if NOT negative, returns 'You have a happy customer')\n",
    "            2. Cleans the string (removes codes, hashtags, emojis, airlines, etc.)\n",
    "            3. Processes words (lemmatization, singularization, etc.)\n",
    "            4. Classifies into one of ... topics\"\"\"\n",
    "    #Defining an action for each issue\n",
    "    action_dict = {'Poor Customer Service': 'Please Improve Customer Service Rep Training \\U0001F61E',\n",
    "                  'Cancellation': \"Please Minimize Cancellation (when in your control) \\U0001F4A8\",\n",
    "                   'Missed Connection': \"Please Improve Your Scheduling \\U0001F553\",\n",
    "                   'Lost Baggage': \"Please Pay More Attention at Baggage Handling \\U0001F45C\",\n",
    "                   'Boarding / Deplaning Issue': 'Please Work on Your Boarding Process \\u2708',\n",
    "                   'Issue with Contacting Customer Service': \"Please Improve Customer Service Accessibility \\u260E\",\n",
    "                   'Bad Weather Related': \"Weather Is Out of Your Control, Relax \\U0001F4A8\",\n",
    "                   'Rebooking Issues': \"Please Improve Your Rebooking Process \\U0001F3AB\",\n",
    "                   'Poor Service from Airport Agent(s)': 'Please Work On Training Your Airport / On Board Staff \\u2708 \\U0001F46B'}\n",
    "    \n",
    "    sentiment = get_sentiment(string)\n",
    "    if sentiment != 'negative':\n",
    "        return 'Keep up the good work! \\U0001F44D'\n",
    "    else:\n",
    "        issue_index = classify_issue(string)\n",
    "        issue = get_issue_name(issue_index)\n",
    "        action = action_dict[issue]\n",
    "    \n",
    "    #Now finally choose the action\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.147Z"
    }
   },
   "outputs": [],
   "source": [
    "random_test_tweet = tweets_test.iloc[np.random.choice(len(tweets_test))]['tweet']\n",
    "print('Test tweet is: \"' + str(random_test_tweet) + '\"')\n",
    "print('The model suggests you to: ' + str(make_suggestion(random_test_tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T18:26:38.149Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_example1 = \"@United I've never had such a bad customer service\"\n",
    "print('Tweet example: '+ str(tweet_example1))\n",
    "print('The model suggests you to: ' + str(make_suggestion(tweet_example1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
