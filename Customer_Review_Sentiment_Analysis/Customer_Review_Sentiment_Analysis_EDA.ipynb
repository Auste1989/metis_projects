{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration, Cleanup and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:18.139056Z",
     "start_time": "2018-08-19T20:53:18.128574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:19.989987Z",
     "start_time": "2018-08-19T20:53:18.143805Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#SQL related\n",
    "import sqlite3\n",
    "import pandas.io.sql as pd_sql\n",
    "\n",
    "#API related\n",
    "import requests\n",
    "\n",
    "#Plotting fun\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Nice to have\n",
    "import seaborn as sns\n",
    "import re\n",
    "from calendar import month_name\n",
    "\n",
    "#NLP modules\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import treebank_chunk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN, SpectralClustering, MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from geotext import GeoText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Neural nets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:19.995225Z",
     "start_time": "2018-08-19T20:53:19.992161Z"
    }
   },
   "outputs": [],
   "source": [
    "#Setting up for working with SQLite database\n",
    "sqlite_file = '/Users/auste_m/ds/metis/metisgh/github/metis_projects/Customer_Review_Sentiment_Analysis/Datasets/twitter-airline-sentiment/database.sqlite'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:20.001975Z",
     "start_time": "2018-08-19T20:53:19.997335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of the table are: \n",
      "['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone']\n",
      "\n",
      "Preview of one of the rows in the table:\n",
      "(567588278875213824, 'neutral', 1, '', '', 'Delta', '', 'JetBlueNews', '', 0, \"@JetBlue's new CEO seeks the right balance to please passengers and Wall ... - Greenfield Daily Reporter http://t.co/LM3opxkxch\", '', '2015-02-16 23:36:05 -0800', 'USA', 'Sydney')\n"
     ]
    }
   ],
   "source": [
    "#Check one of the rows in the table\n",
    "preview = cursor.execute(\"SELECT * FROM Tweets LIMIT 20\")\n",
    "columns = [column[0] for column in preview.description]\n",
    "print('The columns of the table are:' + ' \\n' + str(columns) + '\\n')\n",
    "print('Preview of one of the rows in the table:' + '\\n' + str(preview.fetchone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:20.168190Z",
     "start_time": "2018-08-19T20:53:20.003576Z"
    }
   },
   "outputs": [],
   "source": [
    "#Retrieve relevant information from Tweets table in SQLite database and store them in a pandas dataframe\n",
    "query = \"\"\"SELECT airline, retweet_count, text as 'tweet' \n",
    "            FROM Tweets\"\"\"\n",
    "\n",
    "\n",
    "tweets_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:20.195565Z",
     "start_time": "2018-08-19T20:53:20.169942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14485 entries, 0 to 14484\n",
      "Data columns (total 3 columns):\n",
      "airline          14485 non-null object\n",
      "retweet_count    14485 non-null int64\n",
      "tweet            14485 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 339.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delta</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue's new CEO seeks the right balance to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united yes. We waited in line for almost an h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united the we got into the gate at IAH on tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir its cool that my bags take a bit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline  retweet_count                                              tweet\n",
       "0      Delta              0  @JetBlue's new CEO seeks the right balance to ...\n",
       "1      Delta              0  @JetBlue is REALLY getting on my nerves !! ðŸ˜¡ðŸ˜¡ ...\n",
       "2     United              0  @united yes. We waited in line for almost an h...\n",
       "3     United              0  @united the we got into the gate at IAH on tim...\n",
       "4  Southwest              0  @SouthwestAir its cool that my bags take a bit..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "print(tweets_df.info())\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put aside a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:20.204754Z",
     "start_time": "2018-08-19T20:53:20.197971Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_test = tweets_df[10000:12000]\n",
    "tweets_train = tweets_df[:10000]\n",
    "tweets_train = tweets_train.append(tweets_df[12000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:20.208711Z",
     "start_time": "2018-08-19T20:53:20.206749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tweets_train.info()\n",
    "# tweets_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's gather airport information from an external API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.208519Z",
     "start_time": "2018-08-19T20:53:20.210982Z"
    }
   },
   "outputs": [],
   "source": [
    "#First need to get global airport database through an API request \n",
    "airport_db_url = 'https://aviation-edge.com/api/public/airportDatabase?key=42e87b-a2f1be-c446fa-06d7a2-012f14'\n",
    "get_response = requests.get(airport_db_url)\n",
    "airport_db = get_response.json()\n",
    "\n",
    "#Then I need to extract the information that is relevant to me (airport names and codes)\n",
    "airport_info = []\n",
    "\n",
    "for airport in airport_db:\n",
    "    airport_info.append(airport['codeIataAirport'])\n",
    "    airport_info.append(airport['nameAirport'])\n",
    "\n",
    "#Test that results make sense\n",
    "# if 'IAH' in airport_codes:\n",
    "#     print(airport_db[airport_codes.index('IAH')])\n",
    "# else:\n",
    "#     print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.214584Z",
     "start_time": "2018-08-19T20:53:22.210684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airport_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have some regex fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.228109Z",
     "start_time": "2018-08-19T20:53:22.216631Z"
    }
   },
   "outputs": [],
   "source": [
    "#helper function to remove stuff from tweets\n",
    "\n",
    "def remove_airline(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with hashtag removed.\"\"\"\n",
    "    pattern1 = re.compile('@[A-Za-z]+\\w')\n",
    "    new_string = string\n",
    "    try:\n",
    "        all_airlines = pattern1.findall(new_string)\n",
    "        for airline in all_airlines:\n",
    "            new_string = re.sub(airline, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "def remove_hashtag(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with hashtag removed.\"\"\"\n",
    "    pattern2 = re.compile('#\\w+')\n",
    "    new_string = string\n",
    "    try:\n",
    "        all_hashtags = pattern2.findall(new_string)\n",
    "        for hashtag in all_hashtags:\n",
    "            new_string = re.sub(hashtag, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def remove_code(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with any capital letter & digit combination text removed.\"\"\"\n",
    "    pattern3 = re.compile('[A-Z]?\\d+[A-Z]+')\n",
    "    pattern4 = re.compile('\\d+')\n",
    "    new_string = string\n",
    "    try:\n",
    "        codes = pattern3.findall(new_string)\n",
    "        codes.extend(pattern4.findall(new_string))\n",
    "        for elem in codes:\n",
    "            new_string = re.sub(elem, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "    \n",
    "    \n",
    "def remove_url(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the same string with any urls removed removed.\"\"\"\n",
    "    pattern5 = re.compile('http://t.co/\\w+')\n",
    "    new_string = string\n",
    "    try:\n",
    "        urls = pattern5.findall(new_string)\n",
    "        for url in urls:\n",
    "            new_string = re.sub(url, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def remove_location(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with location information removed.\"\"\"\n",
    "    new_string = string\n",
    "    geo_loc = GeoText(string)\n",
    "    locations = []\n",
    "    if geo_loc.cities != []:\n",
    "        locations.extend(geo_loc.cities)\n",
    "    if geo_loc.countries != []:\n",
    "        locations.extend(geo_loc.countries)\n",
    "    try:\n",
    "        for loc in locations:\n",
    "            new_string = re.sub(loc, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "    \n",
    "    \n",
    "def remove_month(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with month information removed.\"\"\"\n",
    "    new_string = string\n",
    "    all_months = month_name[1:]\n",
    "    try:\n",
    "        for word in string.split():\n",
    "            if word in all_months:\n",
    "                new_string = re.sub(word, '', new_string)\n",
    "            else:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with emojis removed.\"\"\"    \n",
    "    pattern6 = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    new_string = string\n",
    "    try:\n",
    "        emojis = pattern6.findall(new_string)\n",
    "        for emoji in emojis:\n",
    "            new_string = re.sub(emoji, '', new_string)\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n",
    "\n",
    "def remove_airport(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns a new string with airport codes removed.\"\"\" \n",
    "    new_string = string\n",
    "    try:\n",
    "        for word in string.split():\n",
    "            if word in airport_info:\n",
    "                new_string = re.sub(word, '', new_string)\n",
    "            else:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "    return new_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T21:01:51.536234Z",
     "start_time": "2018-08-17T21:01:51.533982Z"
    }
   },
   "source": [
    "#### Examining hashtag containing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.232813Z",
     "start_time": "2018-08-19T20:53:22.230155Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Identify all the tweets containing hashtags\n",
    "# pattern2 = re.compile('#[A-Za-z]+\\w')\n",
    "# count_hash_tweets = 0\n",
    "\n",
    "# for index, tweet in enumerate(tweets_train['tweet']):\n",
    "#     try:\n",
    "#         h_tweet = pattern2.search(tweet).group()\n",
    "#         print(index, h_tweet)\n",
    "#         count_hash_tweets += 1\n",
    "#     except:\n",
    "#         continue\n",
    "        \n",
    "# print('\\nTotal number of tweets containing hashtags =', str(count_hash_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same for urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.236992Z",
     "start_time": "2018-08-19T20:53:22.234301Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Find url pattern\n",
    "# https_list = []\n",
    "\n",
    "# for tweet in tweets_train['tweet']:\n",
    "#     if url_remove(tweet) == []:\n",
    "#         pass\n",
    "#     else:\n",
    "#         https_list.append(url_remove(tweet))\n",
    "        \n",
    "# print(https_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.240859Z",
     "start_time": "2018-08-19T20:53:22.238807Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Testing code_remove function\n",
    "# test_string = tweets_train['tweet'][1582]\n",
    "# print(test_string)\n",
    "# print(url_remove(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wooohooo!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's location time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:53:22.244651Z",
     "start_time": "2018-08-19T20:53:22.242631Z"
    }
   },
   "outputs": [],
   "source": [
    "# madrid_tweet = tweets_train['tweet'][14451]\n",
    "# geo = GeoText(madrid_tweet)\n",
    "# geo.cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-19T20:53:18.279Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clean up tweet column, remove the \"@word\" from the rest of the tweet\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet'].apply(remove_airline)\n",
    "\n",
    "#Clean up tweet column, remove the hashtags from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_hashtag)\n",
    "\n",
    "#Clean up tweet column, remove code-like elements from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_code)\n",
    "\n",
    "#Clean up tweet column, remove urls from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_url)\n",
    "\n",
    "#Clean up tweet column, remove locations from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_location)\n",
    "\n",
    "#Clean up tweet column, remove month names from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_month)\n",
    "\n",
    "#Clean up tweet column, remove emojis from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_emoji)\n",
    "\n",
    "#Clean up tweet column, remove airport codes and names from all tweets\n",
    "tweets_train['tweet_clean'] = tweets_train['tweet_clean'].apply(remove_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini moment of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-19T20:53:18.288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's make sure it works (indexes to test = 1, 3, 1582, 12805, 14451)\n",
    "print(tweets_train['tweet'][1], '\\n')\n",
    "print(tweets_train['tweet_clean'][1], '\\n')\n",
    "\n",
    "print(tweets_train['tweet'][3], '\\n')\n",
    "print(tweets_train['tweet_clean'][3], '\\n')\n",
    "\n",
    "print(tweets_train['tweet'][1582], '\\n')\n",
    "print(tweets_train['tweet_clean'][1582], '\\n')\n",
    "\n",
    "print(tweets_train['tweet'][12805], '\\n')\n",
    "print(tweets_train['tweet_clean'][12805], '\\n')\n",
    "\n",
    "print(tweets_train['tweet'][14451], '\\n')\n",
    "print(tweets_train['tweet_clean'][14451], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-19T20:53:18.299Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-19T20:53:18.353Z"
    }
   },
   "outputs": [],
   "source": [
    "# class RecommendationEngine:\n",
    "    \n",
    "#     def __init__(self, vectorizer, n_components, reducer):\n",
    "#         self.vectorizer = vectorizer\n",
    "#         self.n_dim = n_components\n",
    "#         self.reducer = reducer(n_components)\n",
    "        \n",
    "#     def fit(self, text):\n",
    "#         self.vector_data = self.vectorizer.fit_transform(text)\n",
    "#         self.topic_data = self.reducer.fit_transform(self.vector_data)\n",
    "#         self.text = text\n",
    "        \n",
    "#     def recommend(self, article, num_to_return):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
