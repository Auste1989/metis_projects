{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:13:23.201976Z",
     "start_time": "2018-08-20T21:13:23.190521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:13:28.071429Z",
     "start_time": "2018-08-20T21:13:26.790973Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#Plotting fun\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#NLP fun\n",
    "import nltk\n",
    "from  collections  import namedtuple\n",
    "\n",
    "\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tag import pos_tag\n",
    "# from nltk.chunk import ne_chunk\n",
    "# from nltk.corpus import treebank_chunk\n",
    "# from operator import itemgetter\n",
    "\n",
    "#Dimentionality reduction\n",
    "# from sklearn.decomposition import NMF, TruncatedSVD, PCA, LatentDirichletAllocation\n",
    "\n",
    "#Vectorizing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#NLP modelling\n",
    "# from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, MeanShift, estimate_bandwidth\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.datasets import fetch_mldata\n",
    "\n",
    "\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.neighbors import kneighbors_graph\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#from gensim import corpora, models, similarities, matutils, Word2vec, emoji2vec\n",
    "\n",
    "# #Dim reduction for visualization\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "#Neural nets\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "#Model testing\n",
    "# from sklearn import metrics\n",
    "\n",
    "#Nice to have\n",
    "# from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving forward read from pickle rather than repeat above steps\n",
    "with open('negative_tweets.pkl', 'rb') as picklefile:\n",
    "    negative_tweets = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, it's time to vectorize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some prep work\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer case\n",
    "count_vec = CountVectorizer(stop_words=stop, ngram_range=(1,2))\n",
    "count_vec.fit(neg_tweets)\n",
    "\n",
    "print(vectorizer.get_feature_names(neg_tweets))\n",
    "#neg_tweets_cv = vectorizer.transform(neg_tweets)\n",
    "#neg_tweets_cv = neg_tweets_cv.toarray()\n",
    "#neg_tweets_cv = pd.DataFrame(neg_tweets_cv, columns=count_vex.get_feature_names())\n",
    "#neg_tweets_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF \n",
    "tfidf_vec = TfidfVectorizer(stop_words=stop, ngram_range=(1,2))\n",
    "doc_vectors = tfidf_vec.fit_transform(neg_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Word2vec with google's pre-trianed vectors\n",
    "# import os\n",
    "\n",
    "# google_vec_file = '~/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "# emoji_vec_file = '~/Downloads/emoji2vec.bin'\n",
    "# w2v_google = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RecommendationEngine:\n",
    "    \n",
    "#     def __init__(self, vectorizer, n_components, reducer):\n",
    "#         self.vectorizer = vectorizer\n",
    "#         self.n_dim = n_components\n",
    "#         self.reducer = reducer(n_components)\n",
    "        \n",
    "#     def fit(self, text):\n",
    "#         self.vector_data = self.vectorizer.fit_transform(text)\n",
    "#         self.topic_data = self.reducer.fit_transform(self.vector_data)\n",
    "#         self.text = text\n",
    "        \n",
    "#     def recommend(self, article, num_to_return):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
