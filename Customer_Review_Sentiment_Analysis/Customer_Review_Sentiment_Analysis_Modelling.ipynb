{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:50:01.615981Z",
     "start_time": "2018-08-20T23:50:01.603987Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T03:47:49.666075Z",
     "start_time": "2018-08-21T03:47:45.428142Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "#Plotting fun\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#NLP fun\n",
    "import nltk\n",
    "from  collections  import namedtuple\n",
    "\n",
    "\n",
    "#Text cleaning (stemming, lemmatizing, etc.)\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import inflection\n",
    "from autocorrect import spell\n",
    "\n",
    "\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "# from nltk.chunk import ne_chunk\n",
    "# from nltk.corpus import treebank_chunk\n",
    "# from operator import itemgetter\n",
    "\n",
    "# Dimentionality reduction\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation#, PCA \n",
    "\n",
    "#Vectorizing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "#NLP modelling\n",
    "# from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, MeanShift, estimate_bandwidth\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.datasets import fetch_mldata\n",
    "\n",
    "\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.neighbors import kneighbors_graph\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#from gensim import corpora, models, similarities, matutils, Word2vec, emoji2vec\n",
    "\n",
    "# #Dim reduction for visualization\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "#Neural nets\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "#Model testing\n",
    "# from sklearn import metrics\n",
    "\n",
    "#Nice to have\n",
    "# from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T04:27:55.242313Z",
     "start_time": "2018-08-21T04:27:55.235693Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_pos(pos):\n",
    "    if pos == 'NOUN':\n",
    "        return 'n'\n",
    "    elif pos == 'VERB':\n",
    "        return 'v'\n",
    "    elif pos == 'ADJ':\n",
    "        return 'a'\n",
    "    elif pos == 'ADV':\n",
    "        return 'r'\n",
    "    elif pos == 'ADJ_SAT':\n",
    "        return 's'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def process_words(string):\n",
    "    \"\"\"Takes a string as input.\n",
    "    Returns the string after spell correcting, lemmatizing and singularizing the words.\"\"\"\n",
    "    processed_string = string\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    word_pos_pair = pos_tag(word_tokenize(processed_string), tagset='universal')\n",
    "    for word, pos in word_pos_pair:\n",
    "        word_pos = simple_pos(pos)\n",
    "        spell_correct = ' '+spell(word) if \"'\" in word else spell(word)\n",
    "        lame_word = lmtzr.lemmatize(spell_correct, word_pos)\n",
    "        singular_word = inflection.singularize(lame_word)\n",
    "        processed_string = processed_string.replace(word, singular_word)\n",
    "    return processed_string \n",
    "    \n",
    "    \n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T04:28:11.821726Z",
     "start_time": "2018-08-21T04:28:11.784474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly brilliant flight\n",
      "bleed cancelled flight\n",
      "I like delay less than you because do not the o...\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print(process_words(\"flying brilliant flightled\"))\n",
    "print(process_words(\"bleeming cancelledr flights\"))\n",
    "print(process_words(\"I like delays less than you because don't the o...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T03:12:41.458499Z",
     "start_time": "2018-08-21T03:12:41.448780Z"
    }
   },
   "outputs": [],
   "source": [
    "#Moving forward read from pickle rather than repeat above steps\n",
    "with open('negative_tweets.pkl', 'rb') as picklefile:\n",
    "    negative_tweets = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:50:03.245080Z",
     "start_time": "2018-08-20T23:50:03.229360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>is REALLY getting on my nerves !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "      <td>yes. We waited in line for almost an hour to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United</td>\n",
       "      <td>the we got into the gate at  on time and have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>United</td>\n",
       "      <td>I like delays less than you because I'm the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United</td>\n",
       "      <td>, link to current status of flights/airports? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline                                        tweet_clean\n",
       "1   Delta                is REALLY getting on my nerves !!  \n",
       "2  United   yes. We waited in line for almost an hour to ...\n",
       "3  United   the we got into the gate at  on time and have...\n",
       "6  United   I like delays less than you because I'm the o...\n",
       "7  United  , link to current status of flights/airports? ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets = negative_tweets.drop(columns=['tweet', 'retweet_count', 'sentiment'])\n",
    "negative_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, it's time to vectorize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T03:07:49.383545Z",
     "start_time": "2018-08-21T03:07:43.156810Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some prep work (lemmatize)\n",
    "tweet_col = negative_tweets['tweet_clean']\n",
    "tweet_col = [lemmatize(tweet) for tweet in tweet_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T03:08:08.295619Z",
     "start_time": "2018-08-21T03:08:08.291395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' be REALLY get on my nerve !!  ',\n",
       " ' yes. We wait in line for almost an hour to do so. Some passenger just leave not want to wait past be.',\n",
       " ' the we get into the gate at  on time and have give our seat and close the flight. If you know people be arrive, have to wait',\n",
       " \" I like delay less than you because I'm the one on the plane. Connect me with a voucher\",\n",
       " \", link to current status of flights/airports? Fly BWI-EWR-MCO this morning yet can't yet tell what any problem be except see snow.\",\n",
       " ' I try  DM it would not go thru... not sure why',\n",
       " \" i have item of sentimental value that I'm heartbroken be miss\",\n",
       " ' We have be stick in  for several hour and no one be answer here. Really tough to  SW. No response be bad.',\n",
       " ' be officially the bad, most delay, and least helpful airline I have ever have the mbefortune of fly on',\n",
       " \" won't answer their phone    \"]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_col[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:28.048563Z",
     "start_time": "2018-08-21T00:22:28.044727Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some prep work\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"', '!', ':', 'yes', 'no']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:28.053790Z",
     "start_time": "2018-08-21T00:22:28.050441Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "            'strip_accents': 'ascii',\n",
    "            'lowercase': True,\n",
    "            'stop_words': stop, \n",
    "            'max_df': 0.5,\n",
    "            'min_df': 10,\n",
    "            'ngram_range': (1, 2)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T03:04:03.904097Z",
     "start_time": "2018-08-21T03:04:03.876986Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-a2aa5af85b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Count vectorizer case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweet_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-9e7c422de5e5>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlame_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlmtzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mword_pos_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_pos_pair\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mword_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#Count vectorizer case\n",
    "count_vec = CountVectorizer(**params)\n",
    "\n",
    "negative_tweets_cv = count_vec.fit_transform(tweet_col)\n",
    "negative_tweets_cv = negative_tweets_cv.toarray()\n",
    "negative_tweets_cv = pd.DataFrame(negative_tweets_cv, columns=count_vec.get_feature_names())\n",
    "negative_tweets_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:28.672775Z",
     "start_time": "2018-08-21T00:22:28.368479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>advisory</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wtf</th>\n",
       "      <th>yall</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yr</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  able  absolute  absolutely  acceptable  access  account  actually  \\\n",
       "0  0.0   0.0       0.0         0.0         0.0     0.0      0.0       0.0   \n",
       "1  0.0   0.0       0.0         0.0         0.0     0.0      0.0       0.0   \n",
       "2  0.0   0.0       0.0         0.0         0.0     0.0      0.0       0.0   \n",
       "3  0.0   0.0       0.0         0.0         0.0     0.0      0.0       0.0   \n",
       "4  0.0   0.0       0.0         0.0         0.0     0.0      0.0       0.0   \n",
       "\n",
       "   add  advisory  ...   wrong  wtf  yall  yeah  year  years  yesterday  \\\n",
       "0  0.0       0.0  ...     0.0  0.0   0.0   0.0   0.0    0.0        0.0   \n",
       "1  0.0       0.0  ...     0.0  0.0   0.0   0.0   0.0    0.0        0.0   \n",
       "2  0.0       0.0  ...     0.0  0.0   0.0   0.0   0.0    0.0        0.0   \n",
       "3  0.0       0.0  ...     0.0  0.0   0.0   0.0   0.0    0.0        0.0   \n",
       "4  0.0       0.0  ...     0.0  0.0   0.0   0.0   0.0    0.0        0.0   \n",
       "\n",
       "        yet   yr  zero  \n",
       "0  0.000000  0.0   0.0  \n",
       "1  0.000000  0.0   0.0  \n",
       "2  0.000000  0.0   0.0  \n",
       "3  0.000000  0.0   0.0  \n",
       "4  0.465505  0.0   0.0  \n",
       "\n",
       "[5 rows x 998 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF \n",
    "tfidf_vec = TfidfVectorizer(**params)\n",
    "negative_tweets_tfidf = tfidf_vec.fit_transform(tweet_col)\n",
    "negative_tweets_tfidf = negative_tweets_tfidf.toarray()\n",
    "negative_tweets_tfidf = pd.DataFrame(negative_tweets_tfidf, columns=tfidf_vec.get_feature_names())\n",
    "negative_tweets_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:28.677592Z",
     "start_time": "2018-08-21T00:22:28.674739Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of topics\n",
    "n_topics = 10\n",
    "n_words = 10\n",
    "n_iter = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different dimensionality reduction techniques applied CountVectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:28.966696Z",
     "start_time": "2018-08-21T00:22:28.680064Z"
    }
   },
   "outputs": [],
   "source": [
    "#CountVectorizer + LSA\n",
    "lsa_cv = TruncatedSVD(n_components=n_topics)\n",
    "lsa_cv_data = lsa_cv.fit_transform(negative_tweets_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:28.976251Z",
     "start_time": "2018-08-21T00:22:28.968932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer and PCA:\n",
      "\n",
      "Topic  0\n",
      "flight, cancelled, flightled, cancelled flightled, get, late, delayed, flight cancelled, late flight, hours\n",
      "\n",
      "Topic  1\n",
      "service, customer, customer service, worst, get, amp, ever, terrible, worst customer, flights\n",
      "\n",
      "Topic  2\n",
      "cancelled, cancelled flightled, flightled, flight cancelled, flights, flighted, cancelled flighted, flightled flight, hold, tomorrow\n",
      "\n",
      "Topic  3\n",
      "get, hours, plane, amp, time, late, us, still, back, gate\n",
      "\n",
      "Topic  4\n",
      "late, flightr, late flightr, hours, late flight, still, call, flightled, cancelled flightled, amp\n",
      "\n",
      "Topic  5\n",
      "get, late, late flight, flightr, late flightr, flightled, cancelled flightled, customer, customer service, service\n",
      "\n",
      "Topic  6\n",
      "flighted, cancelled flighted, amp, cancelled, call, late, flighted flight, problems, help, booking problems\n",
      "\n",
      "Topic  7\n",
      "hours, delayed, flighted, cancelled flighted, hold, cancelled, flights, flight delayed, get, flighted flight\n",
      "\n",
      "Topic  8\n",
      "amp, delayed, hours, flight delayed, bag, hold, get, help, delayed hours, need\n",
      "\n",
      "Topic  9\n",
      "hold, hours, problems, time, booking problems, booking, flight booking, phone, call, reflight booking\n"
     ]
    }
   ],
   "source": [
    "print('Count vectorizer and PCA:')\n",
    "display_topics(lsa_cv, count_vec.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:30.397163Z",
     "start_time": "2018-08-21T00:22:28.978588Z"
    }
   },
   "outputs": [],
   "source": [
    "#CountVectorizer + NMF\n",
    "nmf_cv = NMF(n_components=n_topics)\n",
    "nmf_cv_data = nmf_cv.fit_transform(negative_tweets_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:30.406140Z",
     "start_time": "2018-08-21T00:22:30.399423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer and NMF:\n",
      "\n",
      "Topic  0\n",
      "flight, late flight, cancelled flight, problems, booking problems, booking, flight booking, flight flight, flight cancelled, next\n",
      "\n",
      "Topic  1\n",
      "service, customer, customer service, worst, ever, terrible, worst customer, poor, airline, bad\n",
      "\n",
      "Topic  2\n",
      "cancelled, cancelled flightled, flightled, flight cancelled, flightled flight, flights, hold, tomorrow, got, flightled flights\n",
      "\n",
      "Topic  3\n",
      "get, trying, home, trying get, get home, need, phone, back, way, bag\n",
      "\n",
      "Topic  4\n",
      "late, flightr, late flight, late flightr, hours, call, still, hours late, back, hrs\n",
      "\n",
      "Topic  5\n",
      "amp, bag, lost, help, day, still, phone, back, one, call\n",
      "\n",
      "Topic  6\n",
      "cancelled, flighted, cancelled flighted, flights, flight cancelled, hold, flighted flight, weather, cancelled flight, help\n",
      "\n",
      "Topic  7\n",
      "delayed, hours, flight delayed, flights, delayed hours, hold, due, delayed flight, ua, delayed due\n",
      "\n",
      "Topic  8\n",
      "plane, us, gate, hour, delay, waiting, sitting, one, agent, stuck\n",
      "\n",
      "Topic  9\n",
      "time, worst, airline, ever, never, wait, first, fly, flying, last\n"
     ]
    }
   ],
   "source": [
    "print('Count vectorizer and NMF:')\n",
    "display_topics(nmf_cv, count_vec.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:43.763520Z",
     "start_time": "2018-08-21T00:22:30.408782Z"
    }
   },
   "outputs": [],
   "source": [
    "#CountVectorizer + LDA\n",
    "lda_cv = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                    max_iter=n_iter,\n",
    "                                    random_state=42,\n",
    "                                    learning_method='online')\n",
    "\n",
    "lda_cv.fit_transform(negative_tweets_cv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:43.773100Z",
     "start_time": "2018-08-21T00:22:43.765663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer and LDA:\n",
      "\n",
      "Topic  0\n",
      "flight, going, us, call, back, way, got, miss, connection, need\n",
      "\n",
      "Topic  1\n",
      "phone, check, change, amp, online, travel, hrs, flight, told, system\n",
      "\n",
      "Topic  2\n",
      "people, hour, gate, flight, know, rude, agent, boarding, problem, please\n",
      "\n",
      "Topic  3\n",
      "still, waiting, hours, thanks, baggage, minutes, wait, hold, ua, bag\n",
      "\n",
      "Topic  4\n",
      "flight, cancelled, late, delayed, flightled, cancelled flightled, late flight, hours, flighted, cancelled flighted\n",
      "\n",
      "Topic  5\n",
      "flight, flights, cancelled, flying, tomorrow, united, cancelled flight, airlines, airport, tried\n",
      "\n",
      "Topic  6\n",
      "one, plane, take, us, trip, aa, flights, missed, response, times\n",
      "\n",
      "Topic  7\n",
      "service, customer, customer service, bad, weather, getting, ticket, missing, terrible, would\n",
      "\n",
      "Topic  8\n",
      "airline, never, flight, really, worst, problems, time, fly, like, ever\n",
      "\n",
      "Topic  9\n",
      "get, lost, trying, luggage, bag, stuck, day, one, amp, passengers\n"
     ]
    }
   ],
   "source": [
    "print('Count vectorizer and LDA:')\n",
    "display_topics(lda_cv, count_vec.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different dimensionality reduction techniques applied TF-IDFed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:43.874757Z",
     "start_time": "2018-08-21T00:22:43.774773Z"
    }
   },
   "outputs": [],
   "source": [
    "#LSA + TF-IDF\n",
    "lsa_tfidf = TruncatedSVD(n_components=n_topics)\n",
    "lsa_tfidf_data = lsa_tfidf.fit_transform(negative_tweets_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:43.885244Z",
     "start_time": "2018-08-21T00:22:43.877537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF and PCA:\n",
      "\n",
      "Topic  0\n",
      "flight, cancelled, cancelled flightled, flightled, get, service, delayed, flight cancelled, customer, hours\n",
      "\n",
      "Topic  1\n",
      "service, customer, customer service, worst, ever, worst customer, terrible, airline, never, bad\n",
      "\n",
      "Topic  2\n",
      "cancelled, cancelled flightled, flightled, customer, customer service, service, flight cancelled, flightled flight, flights, flighted\n",
      "\n",
      "Topic  3\n",
      "flight, late flight, late, customer service, delayed, customer, service, flight delayed, cancelled flight, booking problems\n",
      "\n",
      "Topic  4\n",
      "worst, airline, ever, delayed, worst airline, never, time, flights, airline ever, experience\n",
      "\n",
      "Topic  5\n",
      "delayed, plane, flight delayed, cancelled flightled, flightled, gate, delay, hours, hour, delayed flight\n",
      "\n",
      "Topic  6\n",
      "flighted, cancelled flighted, problems, booking problems, booking, hold, delayed, hour, flight booking, get\n",
      "\n",
      "Topic  7\n",
      "flighted, cancelled flighted, hours, hold, late, flights, cancelled, plane, flightr, late flightr\n",
      "\n",
      "Topic  8\n",
      "plane, gate, delay, hour, sitting, waiting, flight, worst, hour delay, airline\n",
      "\n",
      "Topic  9\n",
      "hold, hours, hour, get, problems, booking problems, booking, worst, flight booking, hold hour\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF and PCA:')\n",
    "display_topics(lsa_tfidf, tfidf_vec.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:56.883618Z",
     "start_time": "2018-08-21T00:22:43.887583Z"
    }
   },
   "outputs": [],
   "source": [
    "#LDA + TF-IDF\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                    max_iter=n_iter,\n",
    "                                    random_state=42,\n",
    "                                    learning_method='online')\n",
    "\n",
    "lda_tfidf_data = lda_tfidf.fit_transform(negative_tweets_tfidf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:56.893478Z",
     "start_time": "2018-08-21T00:22:56.885675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF and LDA:\n",
      "\n",
      "Topic  0\n",
      "call, hold, help, seriously, someone, ridiculous, hour, back, lost, right\n",
      "\n",
      "Topic  1\n",
      "sorry, well, system, always, wrong, broken, way, fee, customers, nothing\n",
      "\n",
      "Topic  2\n",
      "customer, service, customer service, terrible, agents, response, care, sent, dm, line\n",
      "\n",
      "Topic  3\n",
      "gate, flight, time, still, thanks, flt, crew, rebooked, get, delay\n",
      "\n",
      "Topic  4\n",
      "flight, flighted, cancelled flighted, delayed, miss, cancelled, pay, flight delayed, made, seat\n",
      "\n",
      "Topic  5\n",
      "worst, airline, ever, flying, never, experience, aa, rude, flights, disappointed\n",
      "\n",
      "Topic  6\n",
      "plane, waiting, ua, sitting, bad, tried, people, still, sucks, mins\n",
      "\n",
      "Topic  7\n",
      "late, late flight, flight, flightr, late flightr, mean, miles, everyone, point, idea\n",
      "\n",
      "Topic  8\n",
      "problems, flight, booking, booking problems, leave, website, due, delay, long, weather\n",
      "\n",
      "Topic  9\n",
      "flight, cancelled, cancelled flightled, flightled, get, tomorrow, stuck, flight cancelled, flights, days\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF and LDA:')\n",
    "display_topics(lda_tfidf, tfidf_vec.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:56.898650Z",
     "start_time": "2018-08-21T00:22:56.896387Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Word2vec with google's pre-trianed vectors\n",
    "# import os\n",
    "\n",
    "# google_vec_file = '~/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "# w2v_google = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:22:56.904917Z",
     "start_time": "2018-08-21T00:22:56.900985Z"
    }
   },
   "outputs": [],
   "source": [
    "# class RecommendationEngine:\n",
    "    \n",
    "#     def __init__(self, vectorizer, n_components, reducer):\n",
    "#         self.vectorizer = vectorizer\n",
    "#         self.n_dim = n_components\n",
    "#         self.reducer = reducer(n_components)\n",
    "        \n",
    "#     def fit(self, text):\n",
    "#         self.vector_data = self.vectorizer.fit_transform(text)\n",
    "#         self.topic_data = self.reducer.fit_transform(self.vector_data)\n",
    "#         self.text = text\n",
    "        \n",
    "#     def recommend(self, article, num_to_return):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
