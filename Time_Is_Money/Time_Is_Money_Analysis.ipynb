{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try at getting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:44:55.102283Z",
     "start_time": "2018-09-17T21:44:55.057748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:44:55.203861Z",
     "start_time": "2018-09-17T21:44:55.105878Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import datetime as dt\n",
    "import time as t\n",
    "from termcolor import colored\n",
    "\n",
    "#SQL related\n",
    "import psycopg2\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import pandas.io.sql as pd_sql\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from  statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 25, 6\n",
    "rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the data into an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:44:55.293299Z",
     "start_time": "2018-09-17T21:44:55.207290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the database connection\n",
    "conn = psycopg2.connect(dbname=\"taxi_rides_db\", user=\"auste_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:44:55.302892Z",
     "start_time": "2018-09-17T21:44:55.296073Z"
    }
   },
   "outputs": [],
   "source": [
    "# taxi_rides_query = \"\"\"SELECT * FROM taxi_rides WHERE trip_start_timestamp BETWEEN '2016-02-01' AND '2016-02-15';\"\"\" \n",
    "# weather_query = \"\"\"SELECT * FROM chicago_weather WHERE date BETWEEN '2016-02-01' AND '2016-02-15';\"\"\"\n",
    "\n",
    "taxi_weather_query = \"\"\"SELECT TR.trip_start_timestamp,\n",
    "                                TR.trip_end_timestamp,\n",
    "                                TR.trip_seconds,\n",
    "                                TR.trip_miles,\n",
    "                                CASE WHEN TR.tolls IS NULL THEN TR.fare ELSE TR.fare + TR.tolls END as fare_with_tolls,\n",
    "                                TR.pickup_centroid_latitude,\n",
    "                                TR.pickup_centroid_longitude,\n",
    "                                TR.pickup_centroid_location,\n",
    "                                TR.dropoff_centroid_latitude,\n",
    "                                TR.dropoff_centroid_longitude,\n",
    "                                TR.dropoff_centroid_location,\n",
    "                                CW.avg_daily_wind_speed,\n",
    "                                CW.avg_temp_f,\n",
    "                                CASE WHEN CW.fog = 1 OR CW.heavy_fog = 1 OR ice_fog = 1 OR mist = 1 THEN 1 ELSE 0 END as fog,\n",
    "                                CASE WHEN CW.rain = 1 OR CW.drizzle = 1 OR CW.hail = 1 THEN 1 ELSE 0 END as rain,\n",
    "                                CASE WHEN CW.snow = 1 OR CW.freezing_rain = 1 OR drifting_snow = 1 THEN 1 ELSE 0 END as snow\n",
    "                                \n",
    "                        FROM taxi_rides_clean as TR\n",
    "                        LEFT JOIN chicago_weather as CW\n",
    "                        ON CAST(TR.trip_start_timestamp AS DATE) = CAST(date AS DATE)\n",
    "                        WHERE trip_start_timestamp BETWEEN '2016-01-01' AND '2016-12-31';\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.336730Z",
     "start_time": "2018-09-17T21:44:55.305170Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aef9dfe25ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaxi_weather_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_weather_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    312\u001b[0m     return pandas_sql.read_query(\n\u001b[1;32m    313\u001b[0m         \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                                         parse_dates=parse_dates)\n\u001b[1;32m   1421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetchall_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_fetchall_as_list\u001b[0;34m(self, cur)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetchall_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "taxi_weather_df = pd.read_sql_query(taxi_weather_query, con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.344559Z",
     "start_time": "2018-09-17T21:44:55.061Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Original number of records for 1st-15th February 2016: **831,790**  \n",
    "> Original number of records for the whole of 2016: **19,846,602**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.347426Z",
     "start_time": "2018-09-17T21:44:55.064Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Total number of taxi trips (in mln):', str(round(taxi_weather_df['fare_with_tolls'].count() / 10**6)))\n",
    "print('Total $ spent on taxi trips (in mln):', str(round(taxi_weather_df['fare_with_tolls'].sum() / 10**6)))\n",
    "print('Total number of years of taxi trips:', str(round(taxi_weather_df['trip_seconds'].sum() / 60 / 60 / 24 / 365, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.348792Z",
     "start_time": "2018-09-17T21:44:55.066Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's explore the dataset\n",
    "null_records = taxi_weather_df[(taxi_weather_df['pickup_centroid_latitude'].isnull()) | (taxi_weather_df['pickup_centroid_longitude'].isnull()) | \n",
    "                (taxi_weather_df['dropoff_centroid_latitude'].isnull()) | (taxi_weather_df['dropoff_centroid_longitude'].isnull()) | \n",
    "                (taxi_weather_df['trip_seconds'].isnull()) | (taxi_weather_df['trip_miles'].isnull()) | \n",
    "                (taxi_weather_df['fare_with_tolls'].isnull())]['trip_start_timestamp'].count()\n",
    "\n",
    "print(f'There are {null_records} records with a null value in at least one of the fields mentioned above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeping time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.349988Z",
     "start_time": "2018-09-17T21:44:55.068Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.350802Z",
     "start_time": "2018-09-17T21:44:55.069Z"
    }
   },
   "outputs": [],
   "source": [
    "#Change payment type to boolean\n",
    "taxi_weather_df['payment_type'] = pd.Categorical(taxi_weather_df['payment_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.352487Z",
     "start_time": "2018-09-17T21:44:55.072Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's drop all the records with a missing lat / long in either pickup or dropoff\n",
    "taxi_weather_clean = taxi_weather_df.dropna(subset=['pickup_centroid_latitude', 'pickup_centroid_longitude', 'dropoff_centroid_latitude'\n",
    "                                                    , 'dropoff_centroid_longitude', 'trip_seconds', 'trip_miles', 'fare_with_tolls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.357926Z",
     "start_time": "2018-09-17T21:44:55.073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.360029Z",
     "start_time": "2018-09-17T21:44:55.075Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean[['fog', 'rain', 'snow']] = taxi_weather_clean[['fog', 'rain', 'snow']].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.361941Z",
     "start_time": "2018-09-17T21:44:55.076Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.363324Z",
     "start_time": "2018-09-17T21:44:55.078Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T22:04:14.813734Z",
     "start_time": "2018-09-06T22:04:14.809735Z"
    }
   },
   "source": [
    "> Remaining number of records after removing missing values containing records for 1st-15th February 2016: **81%** (673,474)  \n",
    "> Remaining number of records after removing missing values containing records for whole 2016: **84%** (16,656,816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.367257Z",
     "start_time": "2018-09-17T21:44:55.081Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's get rid of the zeroes\n",
    "taxi_weather_clean = taxi_weather_clean[(taxi_weather_clean['fare_with_tolls'] != 0.0) \n",
    "                                          & (taxi_weather_clean['trip_seconds'] != 0.0) \n",
    "                                          & (taxi_weather_clean['trip_miles'] != 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.369737Z",
     "start_time": "2018-09-17T21:44:55.084Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remaining number of records after null and zero value removal for 1st-15th February 2016: **64%** (533,775)   \n",
    "> Remaining number of records after null and zero value removal for the whole 2016: **67%** (13,354,486)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.373079Z",
     "start_time": "2018-09-17T21:44:55.088Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's have a look if the miles per minute look reasonable\n",
    "plt.figure(dpi=100)\n",
    "plt.scatter(taxi_weather_clean['trip_miles'], taxi_weather_clean['trip_seconds'] / 3600.0)\n",
    "plt.title('Miles per hour')\n",
    "plt.xlabel('Miles')\n",
    "plt.ylabel('Hours');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.375819Z",
     "start_time": "2018-09-17T21:44:55.090Z"
    }
   },
   "outputs": [],
   "source": [
    "miles_per_hour = taxi_weather_clean['trip_miles'] / (taxi_weather_clean['trip_seconds'] / 3600.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.379432Z",
     "start_time": "2018-09-17T21:44:55.091Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "miles_per_hour.hist(bins=10000)\n",
    "plt.xlim(xmin=0.0, xmax=100.0)\n",
    "plt.title('Miles per hour histogram')\n",
    "plt.xlabel('Miles / hour')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.380663Z",
     "start_time": "2018-09-17T21:44:55.093Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing clear error records from the dataset (e.g. $1,000 for 0.1 mile and 6 min ride). Settling at max 100 miles an hour\n",
    "taxi_weather_clean = taxi_weather_clean[(taxi_weather_clean['trip_miles'] / (taxi_weather_clean['trip_seconds'] / 3600.0)) <= 100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.382224Z",
     "start_time": "2018-09-17T21:44:55.094Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Number of records excluded by removing instances with speed higher than 100: **%** ()  \n",
    "> Number of records remaining (whole 2016): **67%** (13,301,862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.383961Z",
     "start_time": "2018-09-17T21:44:55.095Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's have a look if the miles per hour look reasonable\n",
    "plt.figure(dpi=100)\n",
    "plt.scatter(taxi_weather_clean['trip_miles'], taxi_weather_clean['trip_seconds'] / 3600.0)\n",
    "plt.title('Miles per hour (after cleaning)')\n",
    "plt.xlabel('Miles')\n",
    "plt.ylabel('Hours');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.385322Z",
     "start_time": "2018-09-17T21:44:55.096Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's see that's the distribution of $/mile to better identify incorrect values\n",
    "fare_per_mile = taxi_weather_clean['fare_with_tolls'] / taxi_weather_clean['trip_miles']\n",
    "fare_per_minute = taxi_weather_clean['fare_with_tolls'] / (taxi_weather_clean['trip_seconds'] / 60)\n",
    "\n",
    "#Removing obvious outliers\n",
    "outlier_indices = list((fare_per_mile[fare_per_mile > 5000]).index)\n",
    "fare_per_mile = fare_per_mile.drop(index=outlier_indices)\n",
    "fare_per_minute = fare_per_minute.drop(index=outlier_indices)\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.scatter(fare_per_mile, fare_per_minute)\n",
    "plt.title('Fare per mile versus fare per minute')\n",
    "plt.xlabel('Fare/Mile')\n",
    "plt.ylabel('Fare/Minute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.386278Z",
     "start_time": "2018-09-17T21:44:55.098Z"
    }
   },
   "outputs": [],
   "source": [
    "#Based on above analysis, I'm setting a max fare/mile at $250 and max fare/minute to $50\n",
    "taxi_weather_clean = taxi_weather_clean[((taxi_weather_clean['fare_with_tolls'] / taxi_weather_clean['trip_miles']) <= 50) &\n",
    "                                 ((taxi_weather_clean['fare_with_tolls'] / (taxi_weather_clean['trip_seconds'] / 60.0)) <= 250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.387392Z",
     "start_time": "2018-09-17T21:44:55.100Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Number of records excluded by removing instances of fare/mile of more than USD 250 and fare/minute of more than USD 50: **8%** (39,999)  \n",
    "> Number of records remaining: **62%** (12,343,799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.393128Z",
     "start_time": "2018-09-17T21:44:55.106Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's see that's the distribution of $/mile to better identify incorrect values\n",
    "fare_per_mile = taxi_weather_clean['fare_with_tolls'] / taxi_weather_clean['trip_miles']\n",
    "fare_per_minute = taxi_weather_clean['fare_with_tolls'] / (taxi_weather_clean['trip_seconds'] / 60)\n",
    "\n",
    "\n",
    "#Replot to see the effect\n",
    "plt.figure(dpi=100)\n",
    "plt.scatter(fare_per_mile, fare_per_minute)\n",
    "plt.title('Fare per mile versus fare per minute (after cleaning)')\n",
    "plt.xlabel('Fare/Mile')\n",
    "plt.ylabel('Fare/Minute');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.396800Z",
     "start_time": "2018-09-17T21:44:55.109Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's explore the tips and tolls columns\n",
    "taxi_weather_clean.groupby(['payment_type'])['trip_miles'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On average, trips paid with a credit card had longer distance, and therefore, longer duration and higher fare. However, I'm not going to use payment_type as a variable, since it's not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.401493Z",
     "start_time": "2018-09-17T21:44:55.111Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's explore the distribution of variables of interest\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(taxi_weather_clean['trip_miles'], bins=1000, color='m')\n",
    "plt.title('Trip Length Histogram (in miles)')\n",
    "plt.xlim(xmin=0, xmax=30)\n",
    "plt.xlabel('Miles')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(taxi_weather_clean['trip_seconds'], bins=1000, color='c')\n",
    "plt.title('Trip Length Histogram (in seconds)')\n",
    "plt.xlim(xmin=0, xmax=10000)\n",
    "plt.xlabel('Seconds')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(taxi_weather_clean['fare_with_tolls'], bins=5000, color='r')\n",
    "plt.title('Trip Fare (in $)')\n",
    "plt.xlim(xmin=0, xmax=100)\n",
    "plt.xlabel('$')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:00:12.686466Z",
     "start_time": "2018-09-11T21:00:12.684221Z"
    }
   },
   "source": [
    "### What if I log these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.406166Z",
     "start_time": "2018-09-17T21:44:55.116Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's explore the distribution of variables of interest\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(np.log(taxi_weather_clean['trip_miles']), bins=100, color='m')\n",
    "plt.title('Trip Length Log Histogram (in miles)')\n",
    "plt.xlabel('Miles')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(np.log(taxi_weather_clean['trip_seconds']), bins=100, color='c')\n",
    "plt.title('Trip Length Log Histogram (in seconds)')\n",
    "plt.xlabel('Seconds')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(np.log(taxi_weather_clean['fare_with_tolls']), bins=200, color='r')\n",
    "plt.title('Trip Fare Log (in $)')\n",
    "plt.xlabel('$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore different cuts of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.407830Z",
     "start_time": "2018-09-17T21:44:55.118Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean['pickup_date'] = taxi_weather_clean['trip_start_timestamp'].dt.date\n",
    "taxi_weather_clean['pickup_time'] = taxi_weather_clean['trip_start_timestamp'].dt.time\n",
    "# taxi_weather_clean['pickup_time_float'] = [t.mktime(start_time.timetuple()) for start_time in taxi_weather_clean['pickup_time']]\n",
    "taxi_weather_clean['pickup_weekday'] = taxi_weather_clean['trip_start_timestamp'].dt.weekday      #Monday is 0\n",
    "\n",
    "#This is specifically for the heatmap, not used in modelling or elsewhere\n",
    "taxi_weather_clean['pickup_time_formatted'] = [time.strftime(\"%H:%M\") for time in taxi_weather_clean['pickup_time']]    \n",
    "taxi_weather_clean['pickup_weekday_name'] = taxi_weather_clean['trip_start_timestamp'].dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.409411Z",
     "start_time": "2018-09-17T21:44:55.121Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fares by the date variables (day, weekday, time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.411607Z",
     "start_time": "2018-09-17T21:44:55.123Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating pandas series for days, time and weekday\n",
    "avg_pickup_fare_per_mile_over_days = taxi_weather_clean.groupby(['pickup_date'])['fare_with_tolls'].sum() / taxi_weather_clean.groupby(['pickup_date'])['trip_miles'].sum()\n",
    "avg_pickup_fare_per_mile_by_time = taxi_weather_clean.groupby(['pickup_time'])['fare_with_tolls'].sum() / taxi_weather_clean.groupby(['pickup_time'])['trip_miles'].sum()\n",
    "avg_pickup_fare_per_mile_by_weekday = taxi_weather_clean.groupby(['pickup_weekday'])['fare_with_tolls'].sum() / taxi_weather_clean.groupby(['pickup_weekday'])['trip_miles'].sum()\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.417471Z",
     "start_time": "2018-09-17T21:44:55.125Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 20))\n",
    "plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n",
    "                wspace=0.2, hspace=0.3)\n",
    "\n",
    "avg_pickup_fare_per_mile_over_days_R52 = avg_pickup_fare_per_mile_over_days.rolling(52, center=True).mean()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "avg_pickup_fare_per_mile_over_days_R52.plot()\n",
    "plt.title('Average fare per mile over days')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Avg. Fare')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "avg_pickup_fare_per_mile_by_time.plot(color='m')\n",
    "plt.title('Average fare per mile over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Avg. Fare')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "avg_pickup_fare_per_mile_by_weekday.plot(color='c',kind='bar')\n",
    "plt.title('Average fare per mile by weekday')\n",
    "plt.xlabel('Weekday (Monday = 0)')\n",
    "plt.ylabel('Avg. Fare')\n",
    "plt.ylim(3.4, 4.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Average fare across the year has been increasing from January to June, dips in summer then spikes again in October / November and dips again in December. It is strange that the average fare would drop so much in December, since it's an average.  \n",
    "> * Weekday certainly has an impact on the fare, therefore I will be treating the weekday feature as a separate variable in the modelling  \n",
    "> * There's something strange going on in the data between 5am and 7am. Seems like there were a few long distance (high fare) trips around this time. After noticing this, I change the measure from avg. fare to avg. fare per mile.\n",
    "> * Looking at the first two charts, I certainly need to do some differencing to make my time series data more stationary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming in on 5am-7am period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.420996Z",
     "start_time": "2018-09-17T21:44:55.128Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_5_7am = taxi_weather_clean[(taxi_weather_clean['pickup_time'] >= dt.time(5,0,0)) &\n",
    "                                         (taxi_weather_clean['pickup_time'] <= dt.time(7,0,0))]\n",
    "\n",
    "# taxi_weather_5_7am = taxi_weather_5_7am.groupby(['pickup_time'])['fare_with_tolls'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.422861Z",
     "start_time": "2018-09-17T21:44:55.131Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "taxi_weather_5_7am['fare_with_tolls'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.424165Z",
     "start_time": "2018-09-17T21:44:55.134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taxi_weather_5_7am[taxi_weather_5_7am['fare_with_tolls'] >= 1000.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These records seem legitimate, since they have high mileage and hence high fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the demand over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.425927Z",
     "start_time": "2018-09-17T21:44:55.137Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_weather_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.429740Z",
     "start_time": "2018-09-17T21:44:55.139Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating demand dataframe used in time series OLS\n",
    "demand_over_time = taxi_weather_clean.groupby(['trip_start_timestamp'])['trip_start_timestamp'].count()\n",
    "\n",
    "demand_over_days = taxi_weather_clean.groupby(['pickup_date'])['pickup_date'].count()\n",
    "demand_by_15min = taxi_weather_clean.groupby(['pickup_time'])['pickup_time'].count()\n",
    "demand_by_weekday = taxi_weather_clean.groupby(['pickup_weekday'])['pickup_weekday'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.434465Z",
     "start_time": "2018-09-17T21:44:55.140Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 30))\n",
    "plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n",
    "                wspace=0.2, hspace=0.3)\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "demand_over_time.plot(color='c')\n",
    "plt.title('Demand over time')\n",
    "plt.xlabel('Date & Time')\n",
    "plt.ylabel('Number of rides')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "demand_over_days.plot()\n",
    "plt.title('Demand over days')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of rides')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "demand_by_15min.plot(color='m')\n",
    "plt.title('Demand by 15 min intervals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of rides')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "demand_by_weekday.plot(color='c', kind='bar')\n",
    "plt.title('Demand by weekday')\n",
    "plt.xlabel('Weekday (Monday = 0)')\n",
    "plt.ylabel('Number of rides');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is clearly a weekly pattern where Monday and Sunday seem to have the lowest demand (need to verify that, since it's a little surpising)  \n",
    "> There is also a clear pattern in number of rides during the day, with spikes at peak hours, especially the afternoon peak and being slow early in the morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy visualization :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.437437Z",
     "start_time": "2018-09-17T21:44:55.143Z"
    }
   },
   "outputs": [],
   "source": [
    "time_map = taxi_weather_clean.pivot_table(index='pickup_time_formatted', columns=['pickup_weekday'], values='avg_temp_f', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.438618Z",
     "start_time": "2018-09-17T21:44:55.144Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = sns.heatmap(time_map, annot=False, xticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], cmap='coolwarm', cbar_kws={'label':'Taxi Demand'})\n",
    "plt.title('Taxi demand by 15min time period by weekday')\n",
    "ax.set(xlabel='Pickup Weekday', ylabel='Pickup Time');\n",
    "\n",
    "# plt.savefig('Taxi_Demand_Heatmap.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_hm = plt.figure(figsize=(21, 11.7))\n",
    "# fig_hm.patch.set_facecolor('#041F30')\n",
    "\n",
    "# ax = sns.heatmap(time_map, annot=False, xticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], cmap='coolwarm')     # , cbar_kws={'label':'Taxi Demand'})\n",
    "# plt.title('')                                     #Taxi demand by 15min time period by weekday\n",
    "# ax.set(xlabel='', ylabel='');                     #Pickup Weekday     #Pickup Time\n",
    "# ax.tick_params(colors='w', labelcolor='w')\n",
    "# cax = plt.gcf().axes[-1]\n",
    "# cax.tick_params(color='w', labelcolor='w')\n",
    "\n",
    "# plt.savefig('Taxi_Demand_Heatmap_slides.png', facecolor=fig_hm.get_facecolor(), edgecolor=fig_hm.get_edgecolor());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the stationarity of my timeseries data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.440584Z",
     "start_time": "2018-09-17T21:44:55.147Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):   \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    timeseries.dropna(inplace=True)\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "test_stationarity(demand_by_15min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since p-value is higher than 0.05, I cannot reject null hypothesis that the data is not stationare  \n",
    "> The test result suggests that I need to do **differencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.443656Z",
     "start_time": "2018-09-17T21:44:55.150Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Differencing over Days:')\n",
    "print('0 diffs:', demand_over_time.std())\n",
    "print('1 diffs:', demand_over_time.diff().std())\n",
    "print('2 diffs:', demand_over_time.diff().diff().std())\n",
    "print('3 diffs:', demand_over_time.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Differencing **once** seems to be the right call for both dates and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.445621Z",
     "start_time": "2018-09-17T21:44:55.151Z"
    }
   },
   "outputs": [],
   "source": [
    "#So let's do it\n",
    "demand_over_time_diffed = demand_over_time.diff()\n",
    "demand_over_time_diffed = demand_over_time_diffed[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.448287Z",
     "start_time": "2018-09-17T21:44:55.157Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "demand_over_time_diffed.plot(color='c')\n",
    "plt.title('Demand difference over time')\n",
    "plt.xlabel('Date & Time')\n",
    "plt.ylabel(\"Difference from demand 15 minutes ago\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.453213Z",
     "start_time": "2018-09-17T21:44:55.161Z"
    }
   },
   "outputs": [],
   "source": [
    "#What's the stationarity score like after differencing?\n",
    "test_stationarity(demand_over_time_diffed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Much better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now check out how many lags to look back at (remember it's based on 15min time periods)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:05:08.454779Z",
     "start_time": "2018-09-17T21:44:55.163Z"
    }
   },
   "outputs": [],
   "source": [
    "#Partial Autocorrelation plots will help to evaluate how many lags to include as features\n",
    "print(plot_pacf(demand_over_time_diffed, lags=672, title='Partial Autocorrelation (672 lags)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> According to the Partial correlation plot, taking **7 first lags, 96th and 672th lags** is the way to go (last 1h 45min, same time yesterday and same time the week ago)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW I'M READY TO START MODELLING... \n",
    "## ...please see the Time_Is_Money_Demand_Predictor notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
