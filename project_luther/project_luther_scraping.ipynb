{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2. Part 1 (Scrape & Pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Selenium and BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all functions (NEED TO CHANGE THE WHILE LOOK FROM n TO True !!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get urls for all the pages and put them into a list\n",
    "def get_url_list():\n",
    "    \"\"\"Records the current url and goes through the website, clicking Next as many times as there are pages.\n",
    "    Returns a list of urls to be used in the get_htmls function.\"\"\"\n",
    "    #Creates a list of urls for all pages\n",
    "    url_list = []\n",
    "    \n",
    "    #Reads the url of the page the driver is currently in and adds it into the list\n",
    "    current_page_url = driver.current_url\n",
    "    url_list.append(current_page_url)\n",
    "\n",
    "    n = 50\n",
    "    while n > 0:#True:\n",
    "        try:\n",
    "            #Find \"Next\" button and click it\n",
    "            search_button = driver.find_element_by_link_text('Next')\n",
    "            search_button.click()\n",
    "            time.sleep(1)\n",
    "            current_page_url = driver.current_url\n",
    "            url_list.append(current_page_url)\n",
    "            n -= 1\n",
    "        except:\n",
    "            #If \"Next\" button isn't there anymore, return the list\n",
    "            driver.close()\n",
    "            return url_list\n",
    "    driver.close()\n",
    "    return url_list\n",
    "\n",
    "\n",
    "#Get all the html files for each car ad and put it into a list\n",
    "def get_htmls(url, html_list):\n",
    "    \"\"\"Takes an url (of one page) and an existing html list.\n",
    "    Returns updated html list with all the htmls from provided url.\"\"\"\n",
    "    #Convert the url into html and then soup it up\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html5lib')\n",
    "    \n",
    "    #Find all announcement items\n",
    "    all_ads = soup.find_all('a', class_ = 'announcement-item')\n",
    "    \n",
    "    #Copy the existing list of htmls\n",
    "    page_html_list = html_list\n",
    "    \n",
    "    for ad in all_ads:\n",
    "        html = ad.get('href')\n",
    "        page_html_list.append(html)\n",
    "    return page_html_list\n",
    "\n",
    "#Makes the soup from all of the html files\n",
    "def make_soup(html_list):\n",
    "    \"\"\"Generates and return a soup object based on a given html list\"\"\"\n",
    "    soup_list = []\n",
    "    for html in html_list:\n",
    "        source_code = requests.get(html)\n",
    "        soup = BeautifulSoup(source_code.text, 'html5lib')\n",
    "        soup_list.append(soup)\n",
    "    return soup_list\n",
    "\n",
    "\n",
    "#Get the make and engine size for each car\n",
    "def get_make_and_engine(soup):\n",
    "    \"\"\"Given an input soup object retrieves the advert title (containing Make_Model and Engine size)\n",
    "    Returns a dictionary with Make_Model and Engine keys\"\"\"\n",
    "    dividers = soup.find_all('div', {'class', 'col-5 classifieds-info'})\n",
    "    car = {}\n",
    "    for div in dividers:\n",
    "        for title in div('h1'):\n",
    "            title_list = title.text.split(',')\n",
    "            for i in range(3):\n",
    "                if i == 0:\n",
    "                    car['Make_Model'] = title_list[i]\n",
    "                elif i == 1:\n",
    "                    car['Engine_Size'] = title_list[i]\n",
    "    return car    \n",
    "\n",
    "\n",
    "#Get the rest of the features for each car\n",
    "def get_other_features(soup, car_dict):\n",
    "    \"\"\"Retrieves remaining features of a car using input soup object\n",
    "    Returns an updated cars dictionary\"\"\"\n",
    "    car = car_dict\n",
    "    params = soup.find_all('table', {'class', 'announcement-parameters'})\n",
    "    for param in params:\n",
    "        for tr in param('tr'):\n",
    "            for th in tr('th'):\n",
    "                col_title = th.text\n",
    "                for td in tr('td'):\n",
    "                    col_value = td.text\n",
    "                    car[col_title] = col_value\n",
    "    return car\n",
    "\n",
    "\n",
    "#Scrape all the features for all the cars\n",
    "def scrape(soup_list):\n",
    "    \"\"\"Given the soups file this function retrieves all the feature columns about each car\"\"\"\n",
    "    car_dict_list = []\n",
    "    for soup in soup_list:\n",
    "        #Retrieve the car make, model and engine size\n",
    "        car = get_make_and_engine(soup)\n",
    "        #Retrieve the rest of the features\n",
    "        car = get_other_features(soup, car)\n",
    "        car_dict_list.append(car)\n",
    "    return car_dict_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the website\n",
    "website = 'https://en.autoplius.lt/'\n",
    "\n",
    "##Could also have a website list\n",
    "#website_list = ['https://en.autoplius.lt/', 'https://en.autogidas.lt/']\n",
    "#for web in website_list:\n",
    "#    website = web\n",
    "#    driver.get(website)\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\"                     #  path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#Go to the website\n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now filter on cars (not minivans, or trucks)\n",
    "car_type_form = driver.find_element_by_id(\"cats_search_1\")\n",
    "car_type_form.send_keys(\"Cars\")\n",
    "car_type_form.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find \"Search\" button and click it\n",
    "search_button = driver.find_element_by_link_text('Search')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "#Now get all the urls\n",
    "url_list = get_url_list()\n",
    "print(len(url_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n"
     ]
    }
   ],
   "source": [
    "#It's to to get the htmls from each page\n",
    "html_list = []\n",
    "for url in url_list:\n",
    "    get_htmls(url, html_list)\n",
    "\n",
    "print(len(html_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n"
     ]
    }
   ],
   "source": [
    "#Let's make soup (yuck!)\n",
    "soup_list = make_soup(html_list)\n",
    "print(len(soup_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrape the hell out of this website!!!\n",
    "cars_dict = scrape(soup_list)\n",
    "len(cars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create my dataset (pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now fill out the rows\n",
    "car_ad_dataset = pd.DataFrame(cars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pickle the hell out of the result (don't want to have to run this again)\n",
    "with open('car_ad_dataset.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(car_ad_dataset, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('car_ad_dataset.pkl', 'rb') as picklefile:\n",
    "#     my_car_ad_dataset = pickle.load(picklefile)\n",
    "    \n",
    "#print(my_car_ad_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
